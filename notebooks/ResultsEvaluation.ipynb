{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfadbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "\n",
    "import os\n",
    "import json\n",
    "# !pip install pywer\n",
    "import pywer\n",
    "# !pip install pyjarowinkler\n",
    "from pyjarowinkler import distance as jwdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Const:\n",
    "    OCR = 'ocr'\n",
    "    GROUND = 'groundtruth'\n",
    "    REGION = 'region'\n",
    "    LINE = 'line'\n",
    "    SENTENCE = 'sentence'\n",
    "    FILE = 'filename'\n",
    "    DATASET = 'dataset_name'\n",
    "    PREDICTION = 'prediction'\n",
    "    PROMPT = 'prompt'\n",
    "    LANGUAGE = 'language'\n",
    "    NONE = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00f4b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandarallel\n",
    "# !pip install pywer\n",
    "# !pip install pyjarowinkler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ead6e",
   "metadata": {},
   "source": [
    "### Lookup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79f14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/datasets/ocr/converted/icdar-2019/icdar-2019.jsonl icdar-2019\n",
      "../data/datasets/ocr/converted/ajmc-primary/ajmc_primary_text.jsonl ajmc-primary\n",
      "../data/datasets/ocr/converted/overproof/overproof.jsonl overproof\n",
      "../data/datasets/ocr/converted/ajmc-mixed/ajmc_mixed.jsonl ajmc-mixed\n",
      "../data/datasets/ocr/converted/impresso/impresso-nzz.jsonl impresso\n",
      "../data/datasets/ocr/converted/icdar-2017/icdar-2017.jsonl icdar-2017\n",
      "../data/datasets/htr/converted/htrec/htrec.jsonl htrec\n",
      "../data/datasets/asr/converted/ina/ina.jsonl ina\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for type_document in ['ocr', 'htr', 'asr']:\n",
    "    for root, dirs, files in os.walk(f'../data/datasets/{type_document}/converted'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                if 'sample' not in input_file:\n",
    "                    with open(input_file) as f:\n",
    "                        lines = f.read().splitlines()\n",
    "                    df_inter = pd.DataFrame(lines)\n",
    "                    df_inter.columns = ['json_element']\n",
    "                    df_inter['json_element'].apply(json.loads)\n",
    "                    df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    print(input_file, dataset_name)\n",
    "                    df['dataset_name'] = [dataset_name] * len(df)\n",
    "                    if 'ajmc' in dataset_name:\n",
    "                        df['language'] = ['el'] * len(df)\n",
    "                    if 'overproof' in dataset_name:\n",
    "                        df['language'] = ['en'] * len(df)\n",
    "                    if 'impresso' in dataset_name:\n",
    "                        df['language'] = ['de'] * len(df)\n",
    "\n",
    "                    datasets.append(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58611ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lines/sentences/regions.\n",
      "\n",
      "Dataset: icdar-2019 404 with duplicates\n",
      "No. lines: 0 / 404 No. sentences: 404 / 404 No. regions: 41 / 404\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ajmc-primary 57 with duplicates\n",
      "No. lines: 40 / 57 No. sentences: 27 / 57 No. regions: 9 / 57\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: overproof 2669 with duplicates\n",
      "No. lines: 2278 / 2669 No. sentences: 399 / 2669 No. regions: 41 / 2669\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ajmc-mixed 1291 with duplicates\n",
      "No. lines: 535 / 1291 No. sentences: 379 / 1291 No. regions: 33 / 1291\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: impresso 1563 with duplicates\n",
      "No. lines: 1256 / 1563 No. sentences: 577 / 1563 No. regions: 203 / 1563\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: icdar-2017 477 with duplicates\n",
      "No. lines: 0 / 477 No. sentences: 461 / 477 No. regions: 28 / 477\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: htrec 180 with duplicates\n",
      "No. lines: 180 / 180 No. sentences: 8 / 180 No. regions: 8 / 180\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ina 489 with duplicates\n",
      "No. lines: 201 / 489 No. sentences: 290 / 489 No. regions: 6 / 489\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique lines/sentences/regions.\\n')\n",
    "for dataset in datasets:\n",
    "    print('Dataset:', dataset['dataset_name'].unique()[0], len(dataset), 'with duplicates')\n",
    "    print('No. lines:', dataset['ocr.line']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. sentences:', dataset['ocr.sentence']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. regions:', dataset['ocr.region']. nunique(), '/', len(dataset['ocr.region']))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd830ab",
   "metadata": {},
   "source": [
    "## Step 1: Loading of preliminary results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b00aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_by_line(filename):\n",
    "    pass\n",
    "\n",
    "def json_load(text):\n",
    "    \n",
    "    try:\n",
    "        loaded_line = json.loads(text)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(text[:30], '...')\n",
    "        loaded_line = 'No text'\n",
    "    return loaded_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1666da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icdar_2017_lang(filename):\n",
    "    lang = filename.split('/')[-2].split('_')[0]\n",
    "    if lang =='eng':\n",
    "        lang = 'en'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8200754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff1950d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 816/1114 [02:53<01:11,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expecting value: line 1 column 1 (char 0)\n",
      "}\n",
      " ...\n",
      "We could not load ../data/output/few_shot/prompt_complex_01/overproof/results-3few-shot-overproof-facebook-opt-6.7b.jsonl Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1114/1114 [03:49<00:00,  4.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "results = []\n",
    "\n",
    "# First traversal to get count of .jsonl files\n",
    "jsonl_count = 0\n",
    "for root, dirs, files in os.walk('../data/output'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            jsonl_count += 1\n",
    "\n",
    "# Second traversal to do the processing with tqdm progress bar\n",
    "with tqdm(total=jsonl_count) as pbar:\n",
    "    for root, dirs, files in os.walk('../data/output'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                is_few = False\n",
    "                if 'few' in input_file:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    is_few = True\n",
    "                else:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    \n",
    "                if 'sample' not in input_file:\n",
    "#                     if 'prompt_complex_lang' in input_file:\n",
    "#                         print(input_file)\n",
    "#                     prompt = root.split('/')[-2]\n",
    "                    try:\n",
    "                        with open(input_file) as f:\n",
    "                            text = f.read()\n",
    "                    \n",
    "                        with open(input_file) as f:\n",
    "                            lines = f.readlines()\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                        continue\n",
    "                    # Check correct lines\n",
    "                    text = text.replace('\\n', '')\n",
    "                    text_list = text.split('}}{\"')\n",
    "                    json_objects = []\n",
    "\n",
    "                    for i, t in enumerate(text_list):\n",
    "                        if i != 0:\n",
    "                            t = '{\"' + t\n",
    "                        if i != len(text_list) - 1:\n",
    "                            t = t + '}'\n",
    "                        if not t.endswith('}}'):\n",
    "                            json_objects.append(t + '}\\n')\n",
    "                        else:\n",
    "                            json_objects.append(t + '\\n')\n",
    "                        \n",
    "                    df_inter = pd.DataFrame(json_objects)\n",
    "                    df_inter.columns = ['json_element']\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    model_dataset_name = file[8:-6]\n",
    "                    model_name = model_dataset_name.replace(root.split('/')[-1] + '-', '').strip()\n",
    "                    try:\n",
    "                        df_inter['json_element'].apply(lambda x: json_load(x))\n",
    "                        df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                        df['model'] = [model_name] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        df['prompt'] = [prompt] * len(df)\n",
    "                        try:\n",
    "                            with open(f'../data/prompts/{prompt}.txt', 'r') as f:\n",
    "                                prompt_text = f.read()\n",
    "                            prompt_text = prompt\n",
    "                        except:\n",
    "                            prompt_text = 'prompt_complex_03_per_lang'\n",
    "                            \n",
    "                        df['prompt_text'] = [prompt_text] * len(df)\n",
    "                        \n",
    "                        if is_few:\n",
    "                            df['type'] = ['few-shot'] * len(df)\n",
    "                        else:\n",
    "                            df['type'] = ['zero-shot'] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        \n",
    "                        if 'ajmc' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "                        if 'ina' in dataset_name:\n",
    "                            df['language'] = ['fr'] * len(df)\n",
    "                        if 'overproof' in dataset_name:\n",
    "                            df['language'] = ['en'] * len(df)\n",
    "                        if 'impresso' in dataset_name:\n",
    "                            df['language'] = ['de'] * len(df)\n",
    "                        if 'htrec' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "\n",
    "#                         print(dataset_name, model_name, prompt)\n",
    "\n",
    "                        if dataset_name == 'icdar-2017':\n",
    "                            df['language'] = df['filename'].apply(get_icdar_2017_lang)\n",
    "                        \n",
    "                        df['file'] = file\n",
    "                    \n",
    "                        results.append(df)\n",
    "#                         print(df.prompt.unique())\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3223ddaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results-icdar-2019-facebook-opt-6.7b.jsonl'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "834a50a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prompt_complex_03_per_lang'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['prompt_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cdde61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zero-shot'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['type'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f627073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea001bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>filename</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>ocr.line</th>\n",
       "      <th>ocr.sentence</th>\n",
       "      <th>ocr.region</th>\n",
       "      <th>groundtruth.line</th>\n",
       "      <th>groundtruth.sentence</th>\n",
       "      <th>groundtruth.region</th>\n",
       "      <th>prediction.prompt</th>\n",
       "      <th>prediction.sentence</th>\n",
       "      <th>prediction.region</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>../../data/datasets/ocr/original/icdar-2019/IC...</td>\n",
       "      <td>icdar-2019</td>\n",
       "      <td>None</td>\n",
       "      <td>Charles, par la grace de Dieu, etc.</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>None</td>\n",
       "      <td>Charles, par la grace de Dieu, etc.</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>Veuillez nous aider à réviser et à corriger le...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>facebook-opt-6.7b</td>\n",
       "      <td>prompt_complex_lang</td>\n",
       "      <td>prompt_complex_03_per_lang</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>results-icdar-2019-facebook-opt-6.7b.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>../../data/datasets/ocr/original/icdar-2019/IC...</td>\n",
       "      <td>icdar-2019</td>\n",
       "      <td>None</td>\n",
       "      <td>Savoir faisons à touz, presens et avenir, à no...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>None</td>\n",
       "      <td>Savoir faisons à touz, presens et avenir, à no...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>Veuillez nous aider à réviser et à corriger le...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>facebook-opt-6.7b</td>\n",
       "      <td>prompt_complex_lang</td>\n",
       "      <td>prompt_complex_03_per_lang</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>results-icdar-2019-facebook-opt-6.7b.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fr</td>\n",
       "      <td>../../data/datasets/ocr/original/icdar-2019/IC...</td>\n",
       "      <td>icdar-2019</td>\n",
       "      <td>None</td>\n",
       "      <td>Et avant que le dit Amelin peust sur ce remedi...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>None</td>\n",
       "      <td>et avant que le dit Amelin peust sur ce remedi...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>Veuillez nous aider à réviser et à corriger le...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>facebook-opt-6.7b</td>\n",
       "      <td>prompt_complex_lang</td>\n",
       "      <td>prompt_complex_03_per_lang</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>results-icdar-2019-facebook-opt-6.7b.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fr</td>\n",
       "      <td>../../data/datasets/ocr/original/icdar-2019/IC...</td>\n",
       "      <td>icdar-2019</td>\n",
       "      <td>None</td>\n",
       "      <td>linceulz et ii.</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>None</td>\n",
       "      <td>linceulz et n</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>Veuillez nous aider à réviser et à corriger le...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>facebook-opt-6.7b</td>\n",
       "      <td>prompt_complex_lang</td>\n",
       "      <td>prompt_complex_03_per_lang</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>results-icdar-2019-facebook-opt-6.7b.jsonl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fr</td>\n",
       "      <td>../../data/datasets/ocr/original/icdar-2019/IC...</td>\n",
       "      <td>icdar-2019</td>\n",
       "      <td>None</td>\n",
       "      <td>couvertes, et d’ilecques s’en alerent là où bo...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>None</td>\n",
       "      <td>convertes, et d'ilecques s'en alerent là où bo...</td>\n",
       "      <td>Charles, par la grace de Dieu, etc. Savoir fai...</td>\n",
       "      <td>Veuillez nous aider à réviser et à corriger le...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>/s&gt;Veuillez nous aider à réviser et à corriger...</td>\n",
       "      <td>facebook-opt-6.7b</td>\n",
       "      <td>prompt_complex_lang</td>\n",
       "      <td>prompt_complex_03_per_lang</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>results-icdar-2019-facebook-opt-6.7b.jsonl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  language                                           filename dataset_name  \\\n",
       "0       fr  ../../data/datasets/ocr/original/icdar-2019/IC...   icdar-2019   \n",
       "1       fr  ../../data/datasets/ocr/original/icdar-2019/IC...   icdar-2019   \n",
       "2       fr  ../../data/datasets/ocr/original/icdar-2019/IC...   icdar-2019   \n",
       "3       fr  ../../data/datasets/ocr/original/icdar-2019/IC...   icdar-2019   \n",
       "4       fr  ../../data/datasets/ocr/original/icdar-2019/IC...   icdar-2019   \n",
       "\n",
       "  ocr.line                                       ocr.sentence  \\\n",
       "0     None                Charles, par la grace de Dieu, etc.   \n",
       "1     None  Savoir faisons à touz, presens et avenir, à no...   \n",
       "2     None  Et avant que le dit Amelin peust sur ce remedi...   \n",
       "3     None                                    linceulz et ii.   \n",
       "4     None  couvertes, et d’ilecques s’en alerent là où bo...   \n",
       "\n",
       "                                          ocr.region groundtruth.line  \\\n",
       "0  Charles, par la grace de Dieu, etc. Savoir fai...             None   \n",
       "1  Charles, par la grace de Dieu, etc. Savoir fai...             None   \n",
       "2  Charles, par la grace de Dieu, etc. Savoir fai...             None   \n",
       "3  Charles, par la grace de Dieu, etc. Savoir fai...             None   \n",
       "4  Charles, par la grace de Dieu, etc. Savoir fai...             None   \n",
       "\n",
       "                                groundtruth.sentence  \\\n",
       "0                Charles, par la grace de Dieu, etc.   \n",
       "1  Savoir faisons à touz, presens et avenir, à no...   \n",
       "2  et avant que le dit Amelin peust sur ce remedi...   \n",
       "3                                      linceulz et n   \n",
       "4  convertes, et d'ilecques s'en alerent là où bo...   \n",
       "\n",
       "                                  groundtruth.region  \\\n",
       "0  Charles, par la grace de Dieu, etc. Savoir fai...   \n",
       "1  Charles, par la grace de Dieu, etc. Savoir fai...   \n",
       "2  Charles, par la grace de Dieu, etc. Savoir fai...   \n",
       "3  Charles, par la grace de Dieu, etc. Savoir fai...   \n",
       "4  Charles, par la grace de Dieu, etc. Savoir fai...   \n",
       "\n",
       "                                   prediction.prompt  \\\n",
       "0  Veuillez nous aider à réviser et à corriger le...   \n",
       "1  Veuillez nous aider à réviser et à corriger le...   \n",
       "2  Veuillez nous aider à réviser et à corriger le...   \n",
       "3  Veuillez nous aider à réviser et à corriger le...   \n",
       "4  Veuillez nous aider à réviser et à corriger le...   \n",
       "\n",
       "                                 prediction.sentence  \\\n",
       "0  /s>Veuillez nous aider à réviser et à corriger...   \n",
       "1  /s>Veuillez nous aider à réviser et à corriger...   \n",
       "2  /s>Veuillez nous aider à réviser et à corriger...   \n",
       "3  /s>Veuillez nous aider à réviser et à corriger...   \n",
       "4  /s>Veuillez nous aider à réviser et à corriger...   \n",
       "\n",
       "                                   prediction.region              model  \\\n",
       "0  /s>Veuillez nous aider à réviser et à corriger...  facebook-opt-6.7b   \n",
       "1  /s>Veuillez nous aider à réviser et à corriger...  facebook-opt-6.7b   \n",
       "2  /s>Veuillez nous aider à réviser et à corriger...  facebook-opt-6.7b   \n",
       "3  /s>Veuillez nous aider à réviser et à corriger...  facebook-opt-6.7b   \n",
       "4  /s>Veuillez nous aider à réviser et à corriger...  facebook-opt-6.7b   \n",
       "\n",
       "                prompt                 prompt_text       type  \\\n",
       "0  prompt_complex_lang  prompt_complex_03_per_lang  zero-shot   \n",
       "1  prompt_complex_lang  prompt_complex_03_per_lang  zero-shot   \n",
       "2  prompt_complex_lang  prompt_complex_03_per_lang  zero-shot   \n",
       "3  prompt_complex_lang  prompt_complex_03_per_lang  zero-shot   \n",
       "4  prompt_complex_lang  prompt_complex_03_per_lang  zero-shot   \n",
       "\n",
       "                                         file  \n",
       "0  results-icdar-2019-facebook-opt-6.7b.jsonl  \n",
       "1  results-icdar-2019-facebook-opt-6.7b.jsonl  \n",
       "2  results-icdar-2019-facebook-opt-6.7b.jsonl  \n",
       "3  results-icdar-2019-facebook-opt-6.7b.jsonl  \n",
       "4  results-icdar-2019-facebook-opt-6.7b.jsonl  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa0536ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdf5825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mbleven"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08ab9f",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d50ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from Levenshtein import distance\n",
    "from mbleven import compare\n",
    "import distance as faster_distance\n",
    "\n",
    "# def compute_normalized_levenshtein_similarity(ground_truth_text, ocr_text):\n",
    "#     length = max(len(ocr_text), len(ground_truth_text))\n",
    "#     levenshtein_distance = distance(ocr_text, ground_truth_text)\n",
    "#     similarity = (length - levenshtein_distance) / length\n",
    "#     return similarity\n",
    "\n",
    "def compute_normalized_levenshtein_similarity(ground_truth_text, ocr_text):\n",
    "    length = max(len(ocr_text), len(ground_truth_text))\n",
    "#     levenshtein_distance = faster_distance.levenshtein(ocr_text, ground_truth_text, normalized=True)\n",
    "    levenshtein_distance = distance(ocr_text, ground_truth_text)\n",
    "    levenshtein_distance = (length - levenshtein_distance) / length\n",
    "#     return 1.0 - levenshtein_distance\n",
    "    return levenshtein_distance\n",
    "\n",
    "def compute_jaccard(ocr_text, ground_truth_text):\n",
    "    try: \n",
    "        return jwdistance.get_jaro_distance(ocr_text, ground_truth_text)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_improvement(original_similarity, corrected_similarity):\n",
    "    \n",
    "    if original_similarity == 0:\n",
    "        return min(max(corrected_similarity, -1), 1)\n",
    "    elif original_similarity != corrected_similarity:\n",
    "        return min(max((corrected_similarity - original_similarity) / original_similarity, -1), 1)\n",
    "    elif original_similarity == corrected_similarity:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0 if corrected_similarity < 1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f8de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7713953488372093"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = \"149 Obrázek z maloměstského kukátka. Podává L. Gro 149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw\"\n",
    "ocr_text = \"149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw149 Obrázek z maloměstského kukátka. Podává L. Grw\"\n",
    "pred_text = \"\"\"Correct the text: \"149 Obrázek z maloměstského kukátka. Podává L. Grwsmannová-Brodská. Místo děje: salon paní stavitelky; doba: čas věnovaný kávové „visitě\"; jednající osoby: dámy přednější honorace městské, z nichž •většina je mladá a některé skutečně hezké. Na stole prostřeném krásným ubrusem damaškovým stojí talíře s koláčky, věnečky a preclíčky, kol toho pěkně se vyjímají křišťálové sklenky s vodou, stříbrné lžičky a šálky z jemného porcelánu, jejichž vonný obsah na přítomné paničky zdá se velmi blaze působiti. Ze živějšího hovoru vyznívá právě hlas paní notářky, která ve spravedlivém rozhorlení mluví: „Ano, mé dámy, již jest to takové! Samy ráčily jste býti svědky, jak svorně i jednohlasně byl přijat návrh paní sládkové, abychom si pořídily kroje národní a tak přispěly ku zvýšení lesku slavnosti, již pořádá náš statečný studentský spolek „Hvězda\", a když již nás páni akademikové poctili důvěrou, že v naše ruce složili starost o buffet a jiné ještě funkce, tož měly bychom snad též jiti za příkladem slečen berních a vzdáti se činnosti jen proto, že se zdá paní berní národní kroj pro tři dcery býti nějakou zby tečnou výlohou?\" Paničky projevovaly svoji nevoli, každá jiným spůsobem. Mladá paní adjunktová v duchu si umiňovala, že ve svém přátel ství k berňovům trochu ochladne; to tak! aby ten jejich pošetilý nápad, úóinkovati při slavnosti v obyčejném oděvu, přece zvítězil a dámám se bylo odříci těch půvabných krojů venkovských, co by si jen ona, paní adjunktová, počala s tou haldou brokátu, atlasu, krajek, stuh a aksamitu, za což vydala nejednu desítku, utěšujíc se tím, jak jí to bude slušeti! Hm, a škodu z toho také míti nebude, muž se bude musit po několik měsíců uskrovnit, služka se má beztoho též až příliš dobře, uhradí se to na domácnosti a bude! Nyní ujala se slova paní doktorka: „Aj, od berních to není nic divného, považte jen: tolik dětí! Vždyť my všecky víme, že kdyby sobě slečny toillety samy ne řídily, mnohého by nemohly míti; ony pak mají zásadu: nemá-li být něco pěkné, tož raději nic!\" „Pravda, ale slečny Elišky, té nejmladší, jest mně líto; těšila se velice na selský kroj.\" „Ba ano, byla by v něm vypadala roztomile.\" „Nyní má po radosti.\" „Inu, proč má tak nepřející matinku.\" „To není to, má drahá, jest v tom však jiný háček.\" „Ah, ano; vždyť víme, že sotva tak tak vyjdou.\" „Ale na knihy, které jsi tvá, jinak našel byste jen krytí těch mladých mozí, jinak je vydáte.\" „A tak bude, ostatně jdu až na knihy; ale slyšet jsem od někoho, že o krojech něco dělat, a že to bylo bude dělat národní, a když jsi tento národní učil jak, pak by měl nalézt tuto kartu, ta by se jistě mohla vyměnit se všemi. Můžete-li to od nás odkázat?\" Všem ozdravila její hlas, když řekla: „Já, já! a já ho učím. S těmi páne akademiky jsem již mohla vyprávět o světě, v němž vznítí v některé nocy a jednoho dne vyrostá vám hrozný kouzelný strom zůstane, o němž je psáno: ‚Už jen žádat!\" 50 V tuto chvíli koupili mohou-li všichni pánové i paní, že když jsi to čerpala, dá se jen kupit. Přižili to, a já jim vám koupi ukáži.\n",
    "„A já už jste tím vyděštila. Pánové, děkuji vám za chytré vědomí, se kterým vám projeví, že se už točí dívčí zrcadlo.\" – 151\"\"\"\n",
    "                \n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n",
    "\n",
    "# -0.7713953488372093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de31d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9245323586325521"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d29c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = \"302.\"\n",
    "ocr_text = \"302.\"\n",
    "pred_text = \"302.\"\n",
    "\n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021c2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = 'testing'\n",
    "ocr_text = 'reresting'\n",
    "pred_text = 'testing'\n",
    "\n",
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text), distance(ocr_text, gt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab659fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 3, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = 'testing'\n",
    "ocr_text = 'rffesting'\n",
    "\n",
    "compare(ocr_text, gt_text, transpose=True), distance(ocr_text, gt_text), faster_distance.levenshtein(ocr_text, gt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "362c7e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = 'testing'\n",
    "ocr_text = 'resting'\n",
    "\n",
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207d4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5ec1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8e22820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666674"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af90e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 1\n",
      "Levenshtein similarity: 0.86\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# Example strings\n",
    "s1 = \"testing\"\n",
    "s2 = \"resting\"\n",
    "\n",
    "# Calculate the Levenshtein distance\n",
    "lev_distance = Levenshtein.distance(s1, s2)\n",
    "print(f\"Levenshtein distance: {lev_distance}\")\n",
    "\n",
    "# Calculate the Levenshtein similarity\n",
    "similarity = (max(len(s1), len(s2)) - lev_distance) / max(len(s1), len(s2))\n",
    "print(f\"Levenshtein similarity: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1219c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_remove = [\"ΔΙΟΡΘΩΜΈΝΟ ΚΕΊΜΕΝΟ:\", \"CORRECTED TEXT:\", \"КОРИГИРАН ТЕКСТ:\", \"OPRAVENÝ TEXT:\", \n",
    "                   \"KORRIGIERTER TEXT:\",\n",
    "                  \"TEXTO CORREGIDO:\", \"TEXTE CORRIGÉ:\", \"GECORRIGEERDE TEKST:\", \"POPRAWIONY TEKST:\",\n",
    "                  \"POPRAVLJENO BESEDILO:\", 'The corrected text:', 'The corrected text is:', \n",
    "                  'Corrected text:', 'Corrected text is:', \"The correct spelling and grammar are:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c55678d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segment_type(segment_type):\n",
    "    def postprocess(row):\n",
    "        pred_text = row[f'prediction.{segment_type}']\n",
    "        ground_text = row[f'groundtruth.{segment_type}']\n",
    "        prompt_text = row['prompt_text']\n",
    "        \n",
    "        if pred_text is not None:\n",
    "            if type(pred_text) == str:\n",
    "                if len(pred_text.strip()) > 0:\n",
    "                    if pred_text.startswith('\"'):\n",
    "                        pred_text = pred_text[1:]\n",
    "                    if pred_text.endswith('\"'):\n",
    "                        pred_text = pred_text[:-1]\n",
    "                \n",
    "                empty_prompt_text = prompt_text.replace('{{TEXT}}', '').strip()\n",
    "#                 print('PROMP:', empty_prompt_text)\n",
    "                \n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                prompt_text_empty = prompt_text.replace(\"{{TEXT}}\", '').strip()\n",
    "                if prompt_text_empty in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text_empty, '').strip()\n",
    "                prompt_text = prompt_text.replace(\"{{TEXT}}\", pred_text)\n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                pred_text = pred_text.strip()\n",
    "                \n",
    "                \n",
    "                if empty_prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(empty_prompt_text, '')\n",
    "#                     print('Prompt replaced.')\n",
    "\n",
    "                for text in texts_to_remove:\n",
    "                    if text in pred_text:\n",
    "                        pred_text = pred_text[pred_text.index(text)+len(text):].strip()\n",
    "#                         print(ground_text[:40], '----', pred_text[:40], '....')\n",
    "#                         print('--'*20)\n",
    "\n",
    "\n",
    "        return pred_text\n",
    "    return postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69664a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a8bad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed968085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8485cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38d9e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1589662/3664822958.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 2}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n",
    "\n",
    "bleurt.compute(references=[[\"sample text\"]], predictions=[\"sample text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89f37147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [3, 2, 1, 0],\n",
       " 'precisions': [66.66666666666667, 50.0, 50.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 3,\n",
       " 'ref_len': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleurt.compute(references=[[\"sample text\"]], predictions=[\"sample text hjgghj\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "050980fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1589662/2595839807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "results[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58caf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'data/processed_data'\n",
    "os.makedirs(path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced49b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                             | 0/1113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: icdar-2019 Model: facebook-opt-6.7b Prompt: prompt_complex_lang Prompt text: prompt_complex_03_per_lang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                                                                                                                                                                 | 1/1113 [01:02<19:09:33, 62.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to data/processed_data/0_zero-shot_prompt_complex_03_per_lang_icdar-2019_facebook-opt-6.7b.csv\n",
      "Dataset: icdar-2019 Model: gpt-4 Prompt: prompt_complex_lang Prompt text: prompt_complex_03_per_lang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                                                                                                                                                                                 | 2/1113 [02:03<19:00:36, 61.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to data/processed_data/1_zero-shot_prompt_complex_03_per_lang_icdar-2019_gpt-4.csv\n",
      "Dataset: icdar-2019 Model: gpt-3.5-turbo Prompt: prompt_complex_lang Prompt text: prompt_complex_03_per_lang\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▋                                                                                                                                                                                                                                                 | 3/1113 [03:05<19:03:06, 61.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to data/processed_data/2_zero-shot_prompt_complex_03_per_lang_icdar-2019_gpt-3.5-turbo.csv\n",
      "Dataset: icdar-2019 Model: davinci Prompt: prompt_complex_lang Prompt text: prompt_complex_03_per_lang\n"
     ]
    }
   ],
   "source": [
    "for idx, result in tqdm(enumerate(results), total=len(results)):\n",
    "    \n",
    "    try:\n",
    "        results[idx] = results[idx].fillna('No text')\n",
    "    except:\n",
    "        print(idx)\n",
    "        pass\n",
    "    \n",
    "    dataset_name = results[idx]['dataset_name'].unique()[0]\n",
    "    model_name = results[idx]['model'].unique()[0]\n",
    "    prompt = results[idx]['prompt'].unique()[0]\n",
    "    prompt_text = results[idx]['prompt_text'].unique()[0]\n",
    "    run_type = results[idx]['type'].unique()[0]\n",
    "    \n",
    "    print('Dataset:', dataset_name, 'Model:', model_name, 'Prompt:', prompt, 'Prompt text:', prompt_text)\n",
    "    \n",
    "    if 'icdar' in dataset_name:\n",
    "        text_types = ['sentence', 'region']\n",
    "    else:\n",
    "        text_types = ['line', 'sentence', 'region']\n",
    "    for segment_type in text_types:\n",
    "        #try:\n",
    "            results[idx]['length'] = results[idx][f'groundtruth.{segment_type}'].str.len()\n",
    "            results[idx] = results[idx][results[idx]['length'] > 3]\n",
    "\n",
    "            postprocess = postprocess_segment_type(segment_type)\n",
    "            results[idx][f'prediction.{segment_type}'] = results[idx].apply(postprocess, axis=1)\n",
    "\n",
    "            results[idx][f'{segment_type}-lev-ocr'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'ocr.{segment_type}']), axis=1)\n",
    "            \n",
    "        \n",
    "            results[idx][f'{segment_type}-lev-pred'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'prediction.{segment_type}']), axis=1)\n",
    "\n",
    "            results[idx][f'{segment_type}-lev-improvement'] = \\\n",
    "                results[idx].parallel_apply(lambda x: get_improvement(x[f'{segment_type}-lev-ocr'],\n",
    "                                                             x[f'{segment_type}-lev-pred']), axis=1)\n",
    "\n",
    "#             import pdb;pdb.set_trace()\n",
    "#         except Exception as ex:\n",
    "#             print(ex)\n",
    "            \n",
    "    print(f'Saving to data/processed_data/{idx}_{run_type}_{prompt_text}_{dataset_name}_{model_name}.csv')\n",
    "    results[idx].to_csv(f\"data/processed_data/{idx}_{run_type}_{prompt_text}_{dataset_name}_{model_name}.csv\") \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4eb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3cb23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0263e",
   "metadata": {},
   "source": [
    "## Step 2: Generating LEV similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee16de0",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the final results (results concatenation + generating quality bands, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(results)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb9737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "MODEL_MAP = {'gpt-4':'GPT-4', \n",
    "             'gpt-3.5-turbo':'GPT-3.5', \n",
    "             'facebook-opt-350m':'OPT-350M',\n",
    "             'bigscience-bloom-560m':'BLOOM-560M', \n",
    "             'decapoda-research-llama-7b-hf':'LLAMA-7B',\n",
    "             'davinci':'GPT-3', 'gpt2':'GPT-2', \n",
    "             'tloen-alpaca-lora-7b':'Alpaca', \n",
    "             '3few-shot-gpt-4': 'GPT-4', \n",
    "             '3few-shot-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             '3few-shot-davinci': 'GPT-3',\n",
    "             '3few-shot-gpt2': 'GPT-2', \n",
    "             '3few-shot-facebook-opt-350m': 'OPT-350M', \n",
    "             'ajmc_primary_text-gpt-4': 'GPT-4', \n",
    "             'ajmc_primary_text-bigscience-bloom-560m': 'BLOOM-560M', \n",
    "             'ajmc_primary_text-decapoda-research-llama-7b-hf': 'LLAMA-7B', \n",
    "             'ajmc_primary_text-davinci': 'GPT-3', \n",
    "             'ajmc_primary_text-facebook-opt-350m': \"OPT-350M\",\n",
    "             'ajmc_primary_text-gpt2': 'GPT-2',\n",
    "             'ajmc_primary_text-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             'ajmc_mixed-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             'ajmc_mixed-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             'ajmc_mixed-gpt2': 'GPT-2',\n",
    "             'ajmc_mixed-facebook-opt-350m': 'OPT-350M',\n",
    "             'ajmc_mixed-gpt-4': 'GPT-4',\n",
    "             'ajmc_mixed-davinci': 'GPT-3',\n",
    "             'ajmc_mixed-gpt-3.5-turbo': 'GPT-3.5',\n",
    "             '3few-shot-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             '3few-shot-impresso-nzz-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-impresso-nzz-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             '3few-shot-impresso-nzz-facebook-opt-350m': 'OPT-350M',\n",
    "             '3few-shot-..-..-llama-v2-llama-llama-2-7b-': 'LLAMA-2-7B',\n",
    "             '..-..-llama-v2-llama-llama-2-7b-': 'LLAMA-2-7B',\n",
    "             'facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'bigscience-bloom-3b': 'BLOOM-3B',\n",
    "             'bigscience-bloom-7b1': 'BLOOM-7.1B',\n",
    "             '3few-shot-bigscience-bloom-7b1': 'BLOOM-7.1B',\n",
    "             'ina-facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'decapoda-research-llama-13b-hf': 'LLAMA-13B',\n",
    "             'bigscience-bloomz-3b': 'BLOOMZ-3B',\n",
    "             'bigscience-bloomz-7b1': 'BLOOMZ-7.1B',\n",
    "             'bigscience-bloomz-560m': 'BLOOMZ-560M',\n",
    "             '3few-shot-bigscience-bloom-3b': 'BLOOM-3B',\n",
    "             '3few-shot-facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             '3few-shot-bigscience-bloomz-560m': 'BLOOMZ-560M',\n",
    "             '3few-shot-bigscience-bloomz-7b1': 'BLOOMZ-7.1B',\n",
    "             '3few-shot-bigscience-bloomz-3b': 'BLOOMZ-3B',\n",
    "             'meta-llama-Llama-2-7b-hf': 'LLAMA-2-7B',\n",
    "             '3few-shot-meta-llama-Llama-2-7b-hf': 'LLAMA-2-7B',\n",
    "             'decapoda-research-llama-65b-hf': 'LLAMA-65B',\n",
    "             'gpt-3.5-turbo-OLD': 'GPT-3-OLD'\n",
    "            }\n",
    "data['model'] = data['model'].apply(lambda x: MODEL_MAP[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300d6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bbf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46142eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Create a new column for the OCR noise level bins\n",
    "data[f\"Quality Band\"] = pd.cut(data[f'region-lev-ocr'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[f\"Quality Band\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83313819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Overall Levenshtein Improvement\n",
    "data[f'Overall Levenshtein Improvement'] = data[[f'line-lev-improvement', \n",
    "                                                 f'sentence-lev-improvement', \n",
    "                                                 f'region-lev-improvement']].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom binning function\n",
    "def bin_improvement(x):\n",
    "    if x < 0:\n",
    "        return \"Negative Improvement\"\n",
    "    elif x == 0:\n",
    "        return \"No Improvement\"\n",
    "    elif x > 0:\n",
    "        return \"Positive Improvement\"\n",
    "\n",
    "# Apply the function\n",
    "data['Improvement Band'] = data['Overall Levenshtein Improvement'].apply(bin_improvement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b541a3",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c81a",
   "metadata": {},
   "source": [
    "## Sampling for few-shot (commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define five distinct quality bands\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Initialize an empty list for samples\n",
    "# all_samples = []\n",
    "\n",
    "# folder_few_shot = '../data/prompts/few_shot/'\n",
    "\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(['ina'], total=len(['ina'])):\n",
    "# # for dataset in tqdm(data['dataset_name'].unique(), total=len(data['dataset_name'].unique())):\n",
    "    \n",
    "    \n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "    \n",
    "#     print(len(data[data['dataset_name']==dataset]))\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         # Iterate over each unique prompt\n",
    "#         for prompt in prompts:\n",
    "#             sample_list = []\n",
    "            \n",
    "#             output_folder = os.path.join(folder_few_shot, dataset)\n",
    "#             if not os.path.exists(output_folder):\n",
    "#                 os.makedirs(output_folder)\n",
    "            \n",
    "#             # Repeat the sampling process until we have 3 samples\n",
    "#             while len(sample_list) < 3:\n",
    "#                 # Iterate over each unique quality band\n",
    "#                 for quality_band in quality_bands:\n",
    "                    \n",
    "#                     # Filter the data to only include rows that match the current dataset, language, prompt, and quality band\n",
    "#                     subset = data[(data['dataset_name'] == dataset) \n",
    "#                                   & (data['language'] == language)\n",
    "#                                   & (data['prompt'] == prompt)\n",
    "#                                   & (data['Quality Band'] == quality_band)\n",
    "#                                   & (data['prediction.prompt'] != 'No text')\n",
    "#                                   & (data['groundtruth.sentence'].str.len() > 10)]\n",
    "                    \n",
    "#                     # If the subset is not empty and we need more samples, take a sample\n",
    "#                     if not subset.empty and len(sample_list) < 3:\n",
    "#                         sample = subset.sample(1, random_state=1)\n",
    "# #                         print(sample)\n",
    "#                         sample_list.append(sample)\n",
    "#                 # Break the loop if we already have 3 samples\n",
    "#                 if len(sample_list) >= 3:\n",
    "#                     break\n",
    "\n",
    "#             all_samples.extend(sample_list)\n",
    "#             if 'icdar' in dataset_name:\n",
    "#                 text_types = ['sentence', 'region']\n",
    "#             else:\n",
    "#                 text_types = ['line', 'sentence', 'region']\n",
    "            \n",
    "# #             Generating prompts\n",
    "#             print(prompt)\n",
    "#             for segment_type in text_types:\n",
    "#                 output_file = os.path.join(output_folder, f'{prompt}_{segment_type}_{language}.txt')\n",
    "#                 with open(output_file, 'w') as f:\n",
    "#                     for sample in sample_list:\n",
    "#                         sample = sample.iloc[0]\n",
    "#                         prompt_text = sample['prompt_text'].replace('{{TEXT}}', sample[f'ocr.{segment_type}'])\n",
    "#                         correct_text = sample[f'groundtruth.{segment_type}']\n",
    "#                         f.write(f'{prompt_text}\\n\\n{correct_text}\\n\\n')\n",
    "#                         f.write(sample['prompt_text'])\n",
    "\n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(all_samples, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e8925",
   "metadata": {},
   "source": [
    "## Sampling for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4147f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# dataset_names = data.dataset_name.unique()\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# sample_list = []\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(dataset_names, total = len(dataset_names)):\n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     print(dataset)\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         print('  --', language)\n",
    "#         groundtruth_samples = data[(data['dataset_name'] == dataset) \n",
    "#                                    & (data['language'] == language)\n",
    "#                                    & (data['groundtruth.sentence'].str.len() > 10)].drop_duplicates(subset=['groundtruth.line', 'groundtruth.sentence', 'groundtruth.region'])\n",
    "#         # Limit the groundtruth_samples to three\n",
    "#         if len(groundtruth_samples) >= 3:\n",
    "#             groundtruth_samples = groundtruth_samples.sample(3, random_state=1335)\n",
    "\n",
    "            \n",
    "#         print(len(groundtruth_samples))             \n",
    "#         # Iterate over each unique groundtruth samples\n",
    "#         for idx, gt_sample in groundtruth_samples.iterrows():\n",
    "#             prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "                           \n",
    "#             models = data[data['dataset_name'] == dataset]['model'].unique()\n",
    "                          \n",
    "#             improvement_bands = data[data['dataset_name'] == dataset]['Improvement Band'].unique()\n",
    "#             is_few_shot_or_not = data[data['dataset_name'] == dataset]['type'].unique()\n",
    "\n",
    "#             # Iterate over each unique prompt\n",
    "#             for prompt in prompts:\n",
    "#                 print('    -', prompt)\n",
    "#                 # Iterate over each unique model\n",
    "#                 for model in models:\n",
    "# #                     print('     --', model)\n",
    "#                     # Iterate over each quality band\n",
    "#                     for band in quality_bands:\n",
    "# #                         print('        ---', band)\n",
    "#                         # Iterate over each improvement band\n",
    "#                         for improvement_band in improvement_bands:\n",
    "# #                             print('          ----', improvement_band)\n",
    "#                             for is_few_shot in is_few_shot_or_not:\n",
    "# #                                 print('-------', is_few_shot)\n",
    "#                                 subset = data[(data['dataset_name'] == dataset) \n",
    "#                                               & (data['language'] == language)\n",
    "#                                               & (data['prompt'] == prompt)\n",
    "#                                               & (data['model'] == model)\n",
    "#                                               & (data['Improvement Band'] == improvement_band)\n",
    "#                                               & (data['Quality Band'] == band)\n",
    "#                                               & (data['type'] == is_few_shot)\n",
    "#                                               & (data['groundtruth.line'] == gt_sample['groundtruth.line'])\n",
    "#                                               & (data['groundtruth.sentence'] == gt_sample['groundtruth.sentence'])\n",
    "#                                               & (data['groundtruth.region'] == gt_sample['groundtruth.region'])]\n",
    "                                          \n",
    "#                                 # If the subset is not empty, take a sample\n",
    "#                                 if not subset.empty:\n",
    "#                                     sample = subset.sample(1, random_state=1, replace=True)\n",
    "#                                     sample_list.append(sample)\n",
    "#     #                                 print(sample)\n",
    "#     #                             else:\n",
    "#     #                                 print(f\"No samples for Dataset: {dataset}, Language: {language}\")\n",
    "\n",
    "                                      \n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(sample_list, ignore_index=True)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599ac0",
   "metadata": {},
   "source": [
    "### Order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df = sample_df.drop(['length', 'NbAlignedChar', 'prompt_text', 'File'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 'index', \n",
    "# order = ['filename', 'dataset_name', 'model', 'language', 'prompt', \n",
    "#          'Overall Levenshtein Improvement', 'Quality Band', 'Improvement Band',\n",
    "#          'ocr.line', 'groundtruth.line', 'prediction.line', \n",
    "#          'line-lev-ocr', 'line-lev-pred', 'line-lev-improvement',\n",
    "#          'ocr.sentence', 'groundtruth.sentence', 'prediction.sentence', \n",
    "#          'sentence-lev-ocr', 'sentence-lev-pred', 'sentence-lev-improvement', \n",
    "#          'ocr.region', 'groundtruth.region', 'prediction.region',\n",
    "#          'region-lev-ocr', 'region-lev-pred', 'region-lev-improvement', \n",
    "#          'article_id', 'century', 'Date', 'Type']\n",
    "\n",
    "# # Reorder the DataFrame\n",
    "# sample_df = sample_df[order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396a1d",
   "metadata": {},
   "source": [
    "### Write sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Use today's date for the filename\n",
    "# today = datetime.now().strftime('%d%B')  # This will format the date as 'DayMonth'\n",
    "\n",
    "# # Save the DataFrame to a csv file\n",
    "# sample_df.to_csv(f'ResultsGPTUpdated{today}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff79fc4",
   "metadata": {},
   "source": [
    "### Distribution of WER/CER rates for all datasets in the four quality bands, established via Levenshtein similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb6600",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define the bins and labels for quality bands\n",
    "# bins = [0, 0.7, 0.8, 0.9, 1]\n",
    "# labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['cer', 'wer']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Mean {error_rate.upper()}'] = dataset_data[[f'line-{error_rate}-ocr', \n",
    "#                                                  f'sentence-{error_rate}-ocr', \n",
    "#                                                  f'region-{error_rate}-ocr']].mean(axis=1)\n",
    "\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Mean {error_rate.upper()}\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "#         axs[i].set_xlim([0, 100])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         _ = fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f'Mean {error_rate.upper()} Error Rates across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define OCR noise level bins\n",
    "# # bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# # bins = [0, 0.7, 0.8, 0.9, 1.0]\n",
    "# bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# # Assign OCR noise level labels\n",
    "# # labels = [\"0-10%\", \"10-20%\", \"20-30%\", \"30-40%\", \"40-50%\", \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# # labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['lev']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "#                                                           f'sentence-{error_rate}-improvement', \n",
    "#                                                           f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Overall Levenshtein Improvement\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "# #         axs[i].set_ylim([0, 300])\n",
    "#         axs[i].set_xlim([-1, 1])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle('Overall Levenshtein Improvement across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Improvement Band'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['model'] != 'LLaMA-13B']\n",
    "data = data.loc[data['model'] != 'LLAMA-65B']\n",
    "data = data.loc[data['model'] != 'GPT-3-OLD']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "       'LLAMA-7B', 'LLAMA-2-7B', \n",
    "       'BLOOM-560M',  'BLOOM-3B',  'BLOOM-7.1B', \n",
    "       'BLOOMZ-560M', 'BLOOMZ-3B', 'BLOOMZ-7.1B', \n",
    "       'OPT-350M', 'OPT-6.7B',\n",
    "       'GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4']\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a91c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_models = ['LLAMA-7B', 'LLAMA-2-7B', \n",
    "       'BLOOM-560M',  'BLOOM-3B',  'BLOOM-7.1B', \n",
    "       'BLOOMZ-560M', 'BLOOMZ-3B', 'BLOOMZ-7.1B', \n",
    "       'OPT-350M', 'OPT-6.7B',\n",
    "       'GPT-2']  # Replace these with your list of 'limited' models\n",
    "open_models = ['GPT-3', 'GPT-3.5', 'GPT-4']  # Replace these with your list of 'open' models\n",
    "\n",
    "# Create new 'Access' column\n",
    "data['Access'] = data['model'].apply(lambda x: 'limited' if x in limited_models else ('open' if x in open_models else 'unknown'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079a585",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "print(n_figures)\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                      f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1, 1.2])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91890cfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:#\n",
    "    for error_rate in ['lev']:\n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(17, 9))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                    \n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                \n",
    "                dataset_data['model'] = pd.Categorical(dataset_data['model'], categories=MODELS, ordered=True)\n",
    "                \n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                      f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the line plot of improvements for each model\n",
    "                    _ = sns.lineplot(x='model', y=f'Overall Levenshtein Improvement', \n",
    "                                     data=dataset_data.sort_values('model'), \n",
    "                                    ax=axs[i], hue='prompt', sort=False, linewidth=2)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "                    axs[i].set_ylim([-1, 1.2])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)\n",
    "                    axs[i].set_xlabel('Prompt Complexity')  \n",
    "                    axs[i].set_ylabel('Overall Levenshtein Improvement')  \n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Evolution of Prompt Complexity across Datasets, Models, and Improvements ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69825",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each language and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'language', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for language in data.language.unique():  # Iterate over each unique language\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.language == language) & (data.type == type_of_experiment)]  # Filter data for the current language\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and language, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and language again, this time averaging the averages for each language and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the line plot of average improvements for each prompt\n",
    "            _ = sns.lineplot(x='prompt', y=f'Overall Levenshtein Improvement', hue='language', data=average_improvements, \n",
    "                             ax=ax, legend='full')  # Add hue='language' to differentiate lines by language\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement ({type_of_experiment})')  # Modify title since it's no longer specific to one language\n",
    "        ax.set_ylim([-1, 1.2])\n",
    "        ax.set_xlabel('Prompt Complexity')  \n",
    "        ax.set_ylabel('Average Overall Levenshtein Improvement')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each language and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'language', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for language in data.language.unique():  # Iterate over each unique language\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.language == language) & (data.type == type_of_experiment)]  # Filter data for the current language\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and language, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and language again, this time averaging the averages for each language and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the KDE plot of average improvements for each prompt\n",
    "            for language in average_improvements.language.unique():\n",
    "                _ = sns.kdeplot(average_improvements[average_improvements.language == language][f'Overall Levenshtein Improvement'], \n",
    "                                ax=ax, label=language, lw=2.5)  # Increase line width here\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement KDE ({type_of_experiment})')  # Modify title since it's no longer specific to one language\n",
    "        ax.set_xlabel('Average Overall Levenshtein Improvement')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.legend(title='Language', title_fontsize='13', fontsize='12')  # Increase legend fontsize here\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement KDE ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each model and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'model', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for model in data.model.unique():  # Iterate over each unique model\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.model == model) & (data.type == type_of_experiment)]  # Filter data for the current model\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and model, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'model'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and model again, this time averaging the averages for each model and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'model'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the KDE plot of average improvements for each prompt\n",
    "            for model in average_improvements.model.unique():\n",
    "                _ = sns.kdeplot(average_improvements[average_improvements.model == model][f'Overall Levenshtein Improvement'], \n",
    "                                ax=ax, label=model, lw=2.5)  # Increase line width here\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement KDE ({type_of_experiment})')  # Modify title since it's no longer specific to one model\n",
    "        ax.set_xlabel('Average Overall Levenshtein Improvement')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.legend(title='Model', title_fontsize='13', fontsize='12')  # Increase legend fontsize here\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement KDE ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b6985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1.1, 1.1])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfe884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=.7)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "        axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "        for i, prompt in enumerate(prompt_names):\n",
    "            prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "            \n",
    "            if 'icdar' not in dataset:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(prompt_data) > 0:\n",
    "                    sns.kdeplot(data=prompt_data, x=f'Overall Levenshtein Improvement', hue='model', \n",
    "                                fill=True, ax=axs[i], hue_order=MODELS)\n",
    "                    axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                    axs[i].set_xlim([-1, 1.2])\n",
    "                    axs[i].set_ylim([0, 1.4])\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "        # Remove empty subplot\n",
    "        fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, \n",
    "# analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more \n",
    "# dimensions.\n",
    "\n",
    "# Relative to a histogram, KDE can produce a plot that is less cluttered and more interpretable, especially when \n",
    "# drawing multiple distributions. But it has the potential to introduce distortions if the underlying distribution \n",
    "# is bounded or not smooth. Like a histogram, the quality of the representation also depends on the selection of \n",
    "# good smoothing parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd61e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        plt.figure(figsize=(10, 6))  # Adjust as necessary\n",
    "\n",
    "        for language in languages:\n",
    "            language_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt.isin(prompts_to_compare))]\n",
    "\n",
    "            if 'icdar' not in dataset:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(language_data) > 0:\n",
    "                    sns.kdeplot(data=language_data, x=f'Overall Levenshtein Improvement', hue='prompt', \n",
    "                                fill=True, hue_order=prompts_to_compare)\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {language} with {ex}')\n",
    "\n",
    "        plt.title(f'All Languages ({type_of_experiment})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22961951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.)\n",
    "\n",
    "\n",
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "bar_data = []\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for language in languages:\n",
    "            for prompt in prompts_to_compare:\n",
    "                sub_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt == prompt)]\n",
    "                if 'icdar' not in dataset:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                \n",
    "                mean_improvement = np.mean(sub_data[f'Overall Levenshtein Improvement'])\n",
    "                bar_data.append({'Language': language, 'Prompt': prompt, 'Mean Improvement': mean_improvement})\n",
    "\n",
    "bar_data = pd.DataFrame(bar_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Language', y='Mean Improvement', hue='Prompt', data=bar_data, hue_order=prompts_to_compare)\n",
    "plt.title(f'Mean Levenshtein Improvement for all Languages ({type_of_experiment})')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde994d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=.7)\n",
    "\n",
    "segmentations = ['line', 'sentence', 'region']\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for segmentation in segmentations:\n",
    "            fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "            axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "            for i, prompt in enumerate(prompt_names):\n",
    "                prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "                if segmentation == 'line':\n",
    "                    prompt_data = prompt_data[~prompt_data.dataset_name.isin(['icdar-2017', 'icdar-2019'])]\n",
    "                prompt_data[f'{segmentation.capitalize()} Levenshtein Improvement'] = prompt_data[f'{segmentation}-{error_rate}-improvement']\n",
    "\n",
    "                try:\n",
    "                    if len(prompt_data) > 0:\n",
    "                        sns.kdeplot(data=prompt_data, x=f'{segmentation.capitalize()} Levenshtein Improvement', \n",
    "                                    hue='model', fill=True, ax=axs[i], hue_order=MODELS)\n",
    "                        axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                        axs[i].set_xlim([-1, 1.2])\n",
    "                        axs[i].set_ylim([0, 1.5])\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "            # Remove empty subplot\n",
    "            fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "            plt.suptitle(f'{segmentation.capitalize()} Levenshtein Improvement across Prompts and Models ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e54da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results = []  # Define results as a list\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for model in MODELS:\n",
    "        model_data = data[(data.model == model) & (data.type == type_of_experiment)]\n",
    "        \n",
    "        # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "        if 'icdar' not in dataset:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'line-lev-improvement', \n",
    "                                                                        f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "        else:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "        # Append the results\n",
    "#         print(model_data['Improvement Band'].unique())\n",
    "        results.append({'Model': model,\n",
    "                        'Type of Experiment': type_of_experiment,\n",
    "                        'Overall Levenshtein Improvement': np.nanmean(model_data['Overall Levenshtein Improvement'])})\n",
    "\n",
    "# Convert the results list to a DataFrame for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Type of Experiment', data=results_df, order=MODELS)\n",
    "plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba62497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    results = []  # Define results as a list\n",
    "\n",
    "    for model in MODELS:\n",
    "        for band in data['Improvement Band'].unique():\n",
    "            model_band_data = data[(data.model == model) & (data.type == type_of_experiment) & (data['Improvement Band'] == band)]\n",
    "            \n",
    "            # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "            if 'icdar' not in dataset:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'line-lev-improvement', \n",
    "                                                                                    f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "            # Append the results\n",
    "            results.append({'Model': model,\n",
    "                            'Type of Experiment': type_of_experiment,\n",
    "                            'Overall Levenshtein Improvement': np.nanmean(model_band_data['Overall Levenshtein Improvement']),\n",
    "                            'Improvement Band': band})\n",
    "\n",
    "    # Convert the results list to a DataFrame for plotting\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Improvement Band', \n",
    "                data=results_df, order=MODELS)\n",
    "    plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(data, col='Type', hue='Improvement Band', height=10, aspect=1)\n",
    "g.map(sns.barplot, 'model', 'Overall Levenshtein Improvement', order=MODELS)\n",
    "\n",
    "# Calculate means for each type of experiment and add horizontal lines\n",
    "for ax, (type_of_experiment, item) in zip(g.axes.flatten(), data.groupby('Type')):\n",
    "    mean_improvement = item['Overall Levenshtein Improvement'].mean()\n",
    "    ax.axhline(mean_improvement, color='black', linestyle='--')\n",
    "    ax.text(0.6, mean_improvement, f'Mean: {mean_improvement:.2f}', color='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68913b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818014ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b6097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff5018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d142c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
