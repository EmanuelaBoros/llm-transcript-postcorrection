{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfadbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "\n",
    "import os\n",
    "import json\n",
    "# !pip install pywer\n",
    "import pywer\n",
    "# !pip install pyjarowinkler\n",
    "from pyjarowinkler import distance as jwdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Const:\n",
    "    OCR = 'ocr'\n",
    "    GROUND = 'groundtruth'\n",
    "    REGION = 'region'\n",
    "    LINE = 'line'\n",
    "    SENTENCE = 'sentence'\n",
    "    FILE = 'filename'\n",
    "    DATASET = 'dataset_name'\n",
    "    PREDICTION = 'prediction'\n",
    "    PROMPT = 'prompt'\n",
    "    LANGUAGE = 'language'\n",
    "    NONE = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ead6e",
   "metadata": {},
   "source": [
    "### Lookup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f79f14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/datasets/ocr/converted/ajmc-mixed/ajmc_mixed.jsonl ajmc-mixed\n",
      "../data/datasets/ocr/converted/overproof/overproof.jsonl overproof\n",
      "../data/datasets/ocr/converted/icdar-2019/icdar-2019.jsonl icdar-2019\n",
      "../data/datasets/ocr/converted/icdar-2017/icdar-2017.jsonl icdar-2017\n",
      "../data/datasets/ocr/converted/impresso/impresso-nzz.jsonl impresso\n",
      "../data/datasets/ocr/converted/ajmc-primary/ajmc_primary_text.jsonl ajmc-primary\n",
      "../data/datasets/htr/converted/htrec/htrec.jsonl htrec\n",
      "../data/datasets/asr/converted/ina/ina.jsonl ina\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for type_document in ['ocr', 'htr', 'asr']:\n",
    "    for root, dirs, files in os.walk(f'../data/datasets/{type_document}/converted'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                if 'sample' not in input_file:\n",
    "                    with open(input_file) as f:\n",
    "                        lines = f.read().splitlines()\n",
    "                    df_inter = pd.DataFrame(lines)\n",
    "                    df_inter.columns = ['json_element']\n",
    "                    df_inter['json_element'].apply(json.loads)\n",
    "                    df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    print(input_file, dataset_name)\n",
    "                    df['dataset_name'] = [dataset_name] * len(df)\n",
    "                    if 'ajmc' in dataset_name:\n",
    "                        df['language'] = ['el'] * len(df)\n",
    "                    if 'overproof' in dataset_name:\n",
    "                        df['language'] = ['en'] * len(df)\n",
    "                    if 'impresso' in dataset_name:\n",
    "                        df['language'] = ['de'] * len(df)\n",
    "\n",
    "                    datasets.append(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58611ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lines/sentences/regions.\n",
      "\n",
      "Dataset: ajmc-mixed 1291 with duplicates\n",
      "No. lines: 535 / 1291 No. sentences: 379 / 1291 No. regions: 33 / 1291\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: overproof 2669 with duplicates\n",
      "No. lines: 2278 / 2669 No. sentences: 399 / 2669 No. regions: 41 / 2669\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: icdar-2019 404 with duplicates\n",
      "No. lines: 0 / 404 No. sentences: 404 / 404 No. regions: 41 / 404\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: icdar-2017 477 with duplicates\n",
      "No. lines: 0 / 477 No. sentences: 461 / 477 No. regions: 28 / 477\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: impresso 1563 with duplicates\n",
      "No. lines: 1256 / 1563 No. sentences: 577 / 1563 No. regions: 203 / 1563\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ajmc-primary 57 with duplicates\n",
      "No. lines: 40 / 57 No. sentences: 27 / 57 No. regions: 9 / 57\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: htrec 180 with duplicates\n",
      "No. lines: 180 / 180 No. sentences: 8 / 180 No. regions: 8 / 180\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ina 489 with duplicates\n",
      "No. lines: 201 / 489 No. sentences: 290 / 489 No. regions: 6 / 489\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique lines/sentences/regions.\\n')\n",
    "for dataset in datasets:\n",
    "    print('Dataset:', dataset['dataset_name'].unique()[0], len(dataset), 'with duplicates')\n",
    "    print('No. lines:', dataset['ocr.line']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. sentences:', dataset['ocr.sentence']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. regions:', dataset['ocr.region']. nunique(), '/', len(dataset['ocr.region']))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd830ab",
   "metadata": {},
   "source": [
    "## Step 1: Loading of preliminary results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b00aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_by_line(filename):\n",
    "    pass\n",
    "\n",
    "def json_load(text):\n",
    "    \n",
    "    try:\n",
    "        loaded_line = json.loads(text)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(text[:30], '...')\n",
    "        loaded_line = 'No text'\n",
    "    return loaded_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1666da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icdar_2017_lang(filename):\n",
    "    lang = filename.split('/')[-2].split('_')[0]\n",
    "    if lang =='eng':\n",
    "        lang = 'en'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1950d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc; gc.collect()\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "results = []\n",
    "\n",
    "# First traversal to get count of .jsonl files\n",
    "jsonl_count = 0\n",
    "for root, dirs, files in os.walk('../data/output'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            jsonl_count += 1\n",
    "\n",
    "# Second traversal to do the processing with tqdm progress bar\n",
    "with tqdm(total=jsonl_count) as pbar:\n",
    "    for root, dirs, files in os.walk('../data/output'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                is_few = False\n",
    "                if 'few' in input_file:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    is_few = True\n",
    "                else:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    \n",
    "                if 'sample' not in input_file:\n",
    "#                     if 'prompt_complex_lang' in input_file:\n",
    "#                         print(input_file)\n",
    "#                     prompt = root.split('/')[-2]\n",
    "                    try:\n",
    "                        with open(input_file) as f:\n",
    "                            text = f.read()\n",
    "                    \n",
    "                        with open(input_file) as f:\n",
    "                            lines = f.readlines()\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                        continue\n",
    "                    # Check correct lines\n",
    "                    text = text.replace('\\n', '')\n",
    "                    text_list = text.split('}}{\"')\n",
    "                    json_objects = []\n",
    "\n",
    "                    for i, t in enumerate(text_list):\n",
    "                        if i != 0:\n",
    "                            t = '{\"' + t\n",
    "                        if i != len(text_list) - 1:\n",
    "                            t = t + '}'\n",
    "                        if not t.endswith('}}'):\n",
    "                            json_objects.append(t + '}\\n')\n",
    "                        else:\n",
    "                            json_objects.append(t + '\\n')\n",
    "                        \n",
    "                    df_inter = pd.DataFrame(json_objects)\n",
    "                    df_inter.columns = ['json_element']\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    model_dataset_name = file[8:-6]\n",
    "                    model_name = model_dataset_name.replace(root.split('/')[-1] + '-', '').strip()\n",
    "                    try:\n",
    "                        df_inter['json_element'].apply(lambda x: json_load(x))\n",
    "                        df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                        df['model'] = [model_name] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        df['prompt'] = [prompt] * len(df)\n",
    "                        try:\n",
    "                            with open(f'../data/prompts/{prompt}.txt', 'r') as f:\n",
    "                                prompt_text = f.read()\n",
    "                        except:\n",
    "                            prompt_text = 'prompt_complex_03_per_lang'\n",
    "                            \n",
    "                        df['prompt_text'] = [prompt_text] * len(df)\n",
    "                        \n",
    "                        if is_few:\n",
    "                            df['type'] = ['few-shot'] * len(df)\n",
    "                        else:\n",
    "                            df['type'] = ['zero-shot'] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        \n",
    "                        if 'ajmc' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "                        if 'ina' in dataset_name:\n",
    "                            df['language'] = ['fr'] * len(df)\n",
    "                        if 'overproof' in dataset_name:\n",
    "                            df['language'] = ['en'] * len(df)\n",
    "                        if 'impresso' in dataset_name:\n",
    "                            df['language'] = ['de'] * len(df)\n",
    "                        if 'htrec' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "\n",
    "#                         print(dataset_name, model_name, prompt)\n",
    "\n",
    "                        if dataset_name == 'icdar-2017':\n",
    "                            df['language'] = df['filename'].apply(get_icdar_2017_lang)\n",
    "                        \n",
    "                        results.append(df)\n",
    "#                         print(df.prompt.unique())\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223ddaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea001bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08ab9f",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from Levenshtein import distance\n",
    "\n",
    "def compute_normalized_levenshtein_similarity(ocr_text, ground_truth_text):\n",
    "    length = max(len(ocr_text), len(ground_truth_text))\n",
    "    levenshtein_distance = distance(ocr_text, ground_truth_text)\n",
    "    similarity = (length - levenshtein_distance) / length\n",
    "    return similarity\n",
    "\n",
    "def compute_jaccard(ocr_text, ground_truth_text):\n",
    "    try: \n",
    "        return jwdistance.get_jaro_distance(ocr_text, ground_truth_text)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_improvement(original_similarity, corrected_similarity):\n",
    "    \n",
    "    if original_similarity == 0:\n",
    "        return min(max(corrected_similarity, -1), 1)\n",
    "    elif original_similarity != corrected_similarity:\n",
    "        return min(max((corrected_similarity - original_similarity) / original_similarity, -1), 1)\n",
    "    elif original_similarity == corrected_similarity:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0 if corrected_similarity < 1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = \"149 Obrázek z maloměstského kukátka. Podává L. Gro\"\n",
    "ocr_text = \"149 Obrázek z maloměstského kukátka. Podává L. Grw\"\n",
    "pred_text = \"\"\"Correct the text: \"149 Obrázek z maloměstského kukátka. Podává L. Grwsmannová-Brodská. Místo děje: salon paní stavitelky; doba: čas věnovaný kávové „visitě\"; jednající osoby: dámy přednější honorace městské, z nichž •většina je mladá a některé skutečně hezké. Na stole prostřeném krásným ubrusem damaškovým stojí talíře s koláčky, věnečky a preclíčky, kol toho pěkně se vyjímají křišťálové sklenky s vodou, stříbrné lžičky a šálky z jemného porcelánu, jejichž vonný obsah na přítomné paničky zdá se velmi blaze působiti. Ze živějšího hovoru vyznívá právě hlas paní notářky, která ve spravedlivém rozhorlení mluví: „Ano, mé dámy, již jest to takové! Samy ráčily jste býti svědky, jak svorně i jednohlasně byl přijat návrh paní sládkové, abychom si pořídily kroje národní a tak přispěly ku zvýšení lesku slavnosti, již pořádá náš statečný studentský spolek „Hvězda\", a když již nás páni akademikové poctili důvěrou, že v naše ruce složili starost o buffet a jiné ještě funkce, tož měly bychom snad též jiti za příkladem slečen berních a vzdáti se činnosti jen proto, že se zdá paní berní národní kroj pro tři dcery býti nějakou zby tečnou výlohou?\" Paničky projevovaly svoji nevoli, každá jiným spůsobem. Mladá paní adjunktová v duchu si umiňovala, že ve svém přátel ství k berňovům trochu ochladne; to tak! aby ten jejich pošetilý nápad, úóinkovati při slavnosti v obyčejném oděvu, přece zvítězil a dámám se bylo odříci těch půvabných krojů venkovských, co by si jen ona, paní adjunktová, počala s tou haldou brokátu, atlasu, krajek, stuh a aksamitu, za což vydala nejednu desítku, utěšujíc se tím, jak jí to bude slušeti! Hm, a škodu z toho také míti nebude, muž se bude musit po několik měsíců uskrovnit, služka se má beztoho též až příliš dobře, uhradí se to na domácnosti a bude! Nyní ujala se slova paní doktorka: „Aj, od berních to není nic divného, považte jen: tolik dětí! Vždyť my všecky víme, že kdyby sobě slečny toillety samy ne řídily, mnohého by nemohly míti; ony pak mají zásadu: nemá-li být něco pěkné, tož raději nic!\" „Pravda, ale slečny Elišky, té nejmladší, jest mně líto; těšila se velice na selský kroj.\" „Ba ano, byla by v něm vypadala roztomile.\" „Nyní má po radosti.\" „Inu, proč má tak nepřející matinku.\" „To není to, má drahá, jest v tom však jiný háček.\" „Ah, ano; vždyť víme, že sotva tak tak vyjdou.\" „Ale na knihy, které jsi tvá, jinak našel byste jen krytí těch mladých mozí, jinak je vydáte.\" „A tak bude, ostatně jdu až na knihy; ale slyšet jsem od někoho, že o krojech něco dělat, a že to bylo bude dělat národní, a když jsi tento národní učil jak, pak by měl nalézt tuto kartu, ta by se jistě mohla vyměnit se všemi. Můžete-li to od nás odkázat?\" Všem ozdravila její hlas, když řekla: „Já, já! a já ho učím. S těmi páne akademiky jsem již mohla vyprávět o světě, v němž vznítí v některé nocy a jednoho dne vyrostá vám hrozný kouzelný strom zůstane, o němž je psáno: ‚Už jen žádat!\" 50 V tuto chvíli koupili mohou-li všichni pánové i paní, že když jsi to čerpala, dá se jen kupit. Přižili to, a já jim vám koupi ukáži.\n",
    "„A já už jste tím vyděštila. Pánové, děkuji vám za chytré vědomí, se kterým vám projeví, že se už točí dívčí zrcadlo.\" – 151\"\"\"\n",
    "                \n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d29c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = \"302.\"\n",
    "ocr_text = \"302.\"\n",
    "pred_text = \"302.\"\n",
    "\n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_text = 'testing'\n",
    "ocr_text = 'resting'\n",
    "pred_text = 'testing'\n",
    "\n",
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text), distance(gt_text, ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e22820",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90e8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# Example strings\n",
    "s1 = \"testing\"\n",
    "s2 = \"resting\"\n",
    "\n",
    "# Calculate the Levenshtein distance\n",
    "lev_distance = Levenshtein.distance(s1, s2)\n",
    "print(f\"Levenshtein distance: {lev_distance}\")\n",
    "\n",
    "# Calculate the Levenshtein similarity\n",
    "similarity = (max(len(s1), len(s2)) - lev_distance) / max(len(s1), len(s2))\n",
    "print(f\"Levenshtein similarity: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55678d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segment_type(segment_type):\n",
    "    def postprocess(row):\n",
    "        pred_text = row[f'prediction.{segment_type}']\n",
    "        ground_text = row[f'groundtruth.{segment_type}']\n",
    "        prompt_text = row['prompt_text']\n",
    "        \n",
    "        if pred_text is not None:\n",
    "            if type(pred_text) == str:\n",
    "                if len(pred_text.strip()) > 0:\n",
    "                    if pred_text.startswith('\"'):\n",
    "                        pred_text = pred_text[1:]\n",
    "                    if pred_text.endswith('\"'):\n",
    "                        pred_text = pred_text[:-1]\n",
    "                \n",
    "                empty_prompt_text = prompt_text.replace('{{TEXT}}', '').strip()\n",
    "                \n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                prompt_text_empty = prompt_text.replace(\"{{TEXT}}\", '').strip()\n",
    "                if prompt_text_empty in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text_empty, '').strip()\n",
    "#                     print(prompt_text_empty)\n",
    "                prompt_text = prompt_text.replace(\"{{TEXT}}\", pred_text)\n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                pred_text = pred_text.strip()\n",
    "                if empty_prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(empty_prompt_text, '')\n",
    "                    print(empty_prompt_text)\n",
    "                if 'Corrected text:' in pred_text:\n",
    "                    pred_text = pred_text.replace('Corrected text:', '')\n",
    "                if 'Corrected text is:' in pred_text:\n",
    "                    pred_text = pred_text.replace('Corrected text is:', '')\n",
    "                if 'The corrected text:' in pred_text:\n",
    "                    pred_text = pred_text.replace('The corrected text:', '')\n",
    "                if 'The corrected text is:' in pred_text:\n",
    "                    pred_text = pred_text.replace('The corrected text is:', '')\n",
    "                # Cut the pred_text to the length of the ground_text\n",
    "                pred_text = pred_text[:len(ground_text) + len(ground_text)//4].strip()\n",
    "\n",
    "        return pred_text\n",
    "    return postprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0263e",
   "metadata": {},
   "source": [
    "## Step 2: Generating LEV similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced49b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, result in tqdm(enumerate(results), total=len(results)):\n",
    "    \n",
    "    try:\n",
    "        results[idx] = results[idx].fillna('No text')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    dataset_name = results[idx]['dataset_name'].unique()[0]\n",
    "    model_name = results[idx]['model'].unique()[0]\n",
    "    prompt = results[idx]['prompt'].unique()[0]\n",
    "    \n",
    "#     print('Dataset:', dataset_name, 'Model:', model_name, 'Prompt:', prompt)\n",
    "    \n",
    "    if 'icdar' in dataset_name:\n",
    "        text_types = ['sentence', 'region']\n",
    "    else:\n",
    "        text_types = ['line', 'sentence', 'region']\n",
    "    for segment_type in text_types:\n",
    "        try:\n",
    "            results[idx]['length'] = results[idx][f'groundtruth.{segment_type}'].str.len()\n",
    "            results[idx] = results[idx][results[idx]['length'] > 3]\n",
    "\n",
    "            postprocess = postprocess_segment_type(segment_type)\n",
    "            result[f'prediction.{segment_type}'] = result.apply(postprocess, axis=1)\n",
    "\n",
    "            # Compute Lev similarity\n",
    "            results[idx][f'{segment_type}-lev-ocr'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'ocr.{segment_type}']), axis=1)\n",
    "            results[idx][f'{segment_type}-lev-pred'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'prediction.{segment_type}']), axis=1)\n",
    "\n",
    "            results[idx][f'{segment_type}-lev-improvement'] = \\\n",
    "                results[idx].parallel_apply(lambda x: get_improvement(x[f'{segment_type}-lev-ocr'],\n",
    "                                                             x[f'{segment_type}-lev-pred']), axis=1)\n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee16de0",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the final results (results concatenation + generating quality bands, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(results)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAP = {'gpt-4':'GPT-4', \n",
    "             'gpt-3.5-turbo':'GPT-3.5', \n",
    "             'facebook-opt-350m':'OPT-350M',\n",
    "             'bigscience-bloom-560m':'BLOOM-560M', \n",
    "             'decapoda-research-llama-7b-hf':'LLaMA-7B',\n",
    "             'davinci':'GPT-3', 'gpt2':'GPT-2', \n",
    "             'tloen-alpaca-lora-7b':'Alpaca', \n",
    "             '3few-shot-gpt-4': 'GPT-4', \n",
    "             '3few-shot-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             '3few-shot-davinci': 'GPT-3',\n",
    "             '3few-shot-gpt2': 'GPT-2', \n",
    "             '3few-shot-facebook-opt-350m': 'OPT-350M', \n",
    "             'ajmc_primary_text-gpt-4': 'GPT-4', \n",
    "             'ajmc_primary_text-bigscience-bloom-560m': 'BLOOM-560M', \n",
    "             'ajmc_primary_text-decapoda-research-llama-7b-hf': 'LLaMA-7B', \n",
    "             'ajmc_primary_text-davinci': 'GPT-3', \n",
    "             'ajmc_primary_text-facebook-opt-350m': \"OPT-350M\",\n",
    "             'ajmc_primary_text-gpt2': 'GPT-2',\n",
    "             'ajmc_primary_text-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             'ajmc_mixed-decapoda-research-llama-7b-hf': 'LLaMA-7B',\n",
    "             'ajmc_mixed-bigscience-bloom-560m': 'BLOOM',\n",
    "             'ajmc_mixed-gpt2': 'GPT-2',\n",
    "             'ajmc_mixed-facebook-opt-350m': 'OPT-350M',\n",
    "             'ajmc_mixed-gpt-4': 'GPT-4',\n",
    "             'ajmc_mixed-davinci': 'GPT-3',\n",
    "             'ajmc_mixed-gpt-3.5-turbo': 'GPT-3.5',\n",
    "             '3few-shot-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-decapoda-research-llama-7b-hf': 'LLaMA-7B',\n",
    "             '3few-shot-impresso-nzz-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-impresso-nzz-decapoda-research-llama-7b-hf': 'LLaMA-7B',\n",
    "             '3few-shot-impresso-nzz-facebook-opt-350m': 'OPT-350M',\n",
    "             '3few-shot-..-..-llama-v2-llama-llama-2-7b-': 'LLaMA-2-7B',\n",
    "             '..-..-llama-v2-llama-llama-2-7b-': 'LLaMA-2-7B',\n",
    "             'facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'bigscience-bloom-3b': 'BLOOM-3B',\n",
    "             'bigscience-bloom-7b1': 'BLOOM-7.1B',\n",
    "             'ina-facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'decapoda-research-llama-13b-hf': 'LLaMA-13B',\n",
    "             'bigscience-bloomz-3b': 'BLOOMZ-3B',\n",
    "             'bigscience-bloomz-7b1': 'BLOOMZ-7.1B',\n",
    "             'bigscience-bloomz-560m': 'BLOOMZ-560M'\n",
    "            }\n",
    "\n",
    "data['model'] = data['model'].apply(lambda x: MODEL_MAP[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300d6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bbf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['GPT-4', 'GPT-3.5', 'GPT-3', 'GPT-2', \n",
    "          'OPT-350M', 'OPT-6.7B',\n",
    "          'LLaMA-7B', 'LLaMA-13B', 'LLaMA-2-7B', \n",
    "          'BLOOM', 'BLOOM-560M', 'BLOOM-3B', 'BLOOM-7.1B', \n",
    "          'BLOOMZ-560M', 'BLOOMZ-3B', 'BLOOMZ-7.1B']\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46142eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Create a new column for the OCR noise level bins\n",
    "data[f\"Quality Band\"] = pd.cut(data[f'region-lev-ocr'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[f\"Quality Band\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83313819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Overall Levenshtein Improvement\n",
    "data[f'Overall Levenshtein Improvement'] = data[[f'line-lev-improvement', \n",
    "                                                 f'sentence-lev-improvement', \n",
    "                                                 f'region-lev-improvement']].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom binning function\n",
    "def bin_improvement(x):\n",
    "    if x < 0:\n",
    "        return \"Negative Improvement\"\n",
    "    elif x == 0:\n",
    "        return \"No Improvement\"\n",
    "    elif x > 0:\n",
    "        return \"Positive Improvement\"\n",
    "\n",
    "# Apply the function\n",
    "data['Improvement Band'] = data['Overall Levenshtein Improvement'].apply(bin_improvement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b541a3",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c81a",
   "metadata": {},
   "source": [
    "## Sampling for few-shot (commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define five distinct quality bands\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Initialize an empty list for samples\n",
    "# all_samples = []\n",
    "\n",
    "# folder_few_shot = '../data/prompts/few_shot/'\n",
    "\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(['ina'], total=len(['ina'])):\n",
    "# # for dataset in tqdm(data['dataset_name'].unique(), total=len(data['dataset_name'].unique())):\n",
    "    \n",
    "    \n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "    \n",
    "#     print(len(data[data['dataset_name']==dataset]))\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         # Iterate over each unique prompt\n",
    "#         for prompt in prompts:\n",
    "#             sample_list = []\n",
    "            \n",
    "#             output_folder = os.path.join(folder_few_shot, dataset)\n",
    "#             if not os.path.exists(output_folder):\n",
    "#                 os.makedirs(output_folder)\n",
    "            \n",
    "#             # Repeat the sampling process until we have 3 samples\n",
    "#             while len(sample_list) < 3:\n",
    "#                 # Iterate over each unique quality band\n",
    "#                 for quality_band in quality_bands:\n",
    "                    \n",
    "#                     # Filter the data to only include rows that match the current dataset, language, prompt, and quality band\n",
    "#                     subset = data[(data['dataset_name'] == dataset) \n",
    "#                                   & (data['language'] == language)\n",
    "#                                   & (data['prompt'] == prompt)\n",
    "#                                   & (data['Quality Band'] == quality_band)\n",
    "#                                   & (data['prediction.prompt'] != 'No text')\n",
    "#                                   & (data['groundtruth.sentence'].str.len() > 10)]\n",
    "                    \n",
    "#                     # If the subset is not empty and we need more samples, take a sample\n",
    "#                     if not subset.empty and len(sample_list) < 3:\n",
    "#                         sample = subset.sample(1, random_state=1)\n",
    "# #                         print(sample)\n",
    "#                         sample_list.append(sample)\n",
    "#                 # Break the loop if we already have 3 samples\n",
    "#                 if len(sample_list) >= 3:\n",
    "#                     break\n",
    "\n",
    "#             all_samples.extend(sample_list)\n",
    "#             if 'icdar' in dataset_name:\n",
    "#                 text_types = ['sentence', 'region']\n",
    "#             else:\n",
    "#                 text_types = ['line', 'sentence', 'region']\n",
    "            \n",
    "# #             Generating prompts\n",
    "#             print(prompt)\n",
    "#             for segment_type in text_types:\n",
    "#                 output_file = os.path.join(output_folder, f'{prompt}_{segment_type}_{language}.txt')\n",
    "#                 with open(output_file, 'w') as f:\n",
    "#                     for sample in sample_list:\n",
    "#                         sample = sample.iloc[0]\n",
    "#                         prompt_text = sample['prompt_text'].replace('{{TEXT}}', sample[f'ocr.{segment_type}'])\n",
    "#                         correct_text = sample[f'groundtruth.{segment_type}']\n",
    "#                         f.write(f'{prompt_text}\\n\\n{correct_text}\\n\\n')\n",
    "#                         f.write(sample['prompt_text'])\n",
    "\n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(all_samples, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e8925",
   "metadata": {},
   "source": [
    "## Sampling for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4147f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# dataset_names = data.dataset_name.unique()\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# sample_list = []\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(dataset_names, total = len(dataset_names)):\n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     print(dataset)\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         print('  --', language)\n",
    "#         groundtruth_samples = data[(data['dataset_name'] == dataset) \n",
    "#                                    & (data['language'] == language)\n",
    "#                                    & (data['groundtruth.sentence'].str.len() > 10)].drop_duplicates(subset=['groundtruth.line', 'groundtruth.sentence', 'groundtruth.region'])\n",
    "#         # Limit the groundtruth_samples to three\n",
    "#         if len(groundtruth_samples) >= 3:\n",
    "#             groundtruth_samples = groundtruth_samples.sample(3, random_state=1335)\n",
    "\n",
    "            \n",
    "#         print(len(groundtruth_samples))             \n",
    "#         # Iterate over each unique groundtruth samples\n",
    "#         for idx, gt_sample in groundtruth_samples.iterrows():\n",
    "#             prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "                           \n",
    "#             models = data[data['dataset_name'] == dataset]['model'].unique()\n",
    "                          \n",
    "#             improvement_bands = data[data['dataset_name'] == dataset]['Improvement Band'].unique()\n",
    "#             is_few_shot_or_not = data[data['dataset_name'] == dataset]['type'].unique()\n",
    "\n",
    "#             # Iterate over each unique prompt\n",
    "#             for prompt in prompts:\n",
    "#                 print('    -', prompt)\n",
    "#                 # Iterate over each unique model\n",
    "#                 for model in models:\n",
    "# #                     print('     --', model)\n",
    "#                     # Iterate over each quality band\n",
    "#                     for band in quality_bands:\n",
    "# #                         print('        ---', band)\n",
    "#                         # Iterate over each improvement band\n",
    "#                         for improvement_band in improvement_bands:\n",
    "# #                             print('          ----', improvement_band)\n",
    "#                             for is_few_shot in is_few_shot_or_not:\n",
    "# #                                 print('-------', is_few_shot)\n",
    "#                                 subset = data[(data['dataset_name'] == dataset) \n",
    "#                                               & (data['language'] == language)\n",
    "#                                               & (data['prompt'] == prompt)\n",
    "#                                               & (data['model'] == model)\n",
    "#                                               & (data['Improvement Band'] == improvement_band)\n",
    "#                                               & (data['Quality Band'] == band)\n",
    "#                                               & (data['type'] == is_few_shot)\n",
    "#                                               & (data['groundtruth.line'] == gt_sample['groundtruth.line'])\n",
    "#                                               & (data['groundtruth.sentence'] == gt_sample['groundtruth.sentence'])\n",
    "#                                               & (data['groundtruth.region'] == gt_sample['groundtruth.region'])]\n",
    "                                          \n",
    "#                                 # If the subset is not empty, take a sample\n",
    "#                                 if not subset.empty:\n",
    "#                                     sample = subset.sample(1, random_state=1, replace=True)\n",
    "#                                     sample_list.append(sample)\n",
    "#     #                                 print(sample)\n",
    "#     #                             else:\n",
    "#     #                                 print(f\"No samples for Dataset: {dataset}, Language: {language}\")\n",
    "\n",
    "                                      \n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(sample_list, ignore_index=True)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599ac0",
   "metadata": {},
   "source": [
    "### Order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.drop(['length', 'NbAlignedChar', 'prompt_text', 'File'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'index', \n",
    "order = ['filename', 'dataset_name', 'model', 'language', 'prompt', \n",
    "         'Overall Levenshtein Improvement', 'Quality Band', 'Improvement Band',\n",
    "         'ocr.line', 'groundtruth.line', 'prediction.line', \n",
    "         'line-lev-ocr', 'line-lev-pred', 'line-lev-improvement',\n",
    "         'ocr.sentence', 'groundtruth.sentence', 'prediction.sentence', \n",
    "         'sentence-lev-ocr', 'sentence-lev-pred', 'sentence-lev-improvement', \n",
    "         'ocr.region', 'groundtruth.region', 'prediction.region',\n",
    "         'region-lev-ocr', 'region-lev-pred', 'region-lev-improvement', \n",
    "         'article_id', 'century', 'Date', 'Type']\n",
    "\n",
    "# Reorder the DataFrame\n",
    "sample_df = sample_df[order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396a1d",
   "metadata": {},
   "source": [
    "### Write sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Use today's date for the filename\n",
    "today = datetime.now().strftime('%d%B')  # This will format the date as 'DayMonth'\n",
    "\n",
    "# Save the DataFrame to a csv file\n",
    "sample_df.to_csv(f'ResultsGPTUpdated{today}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff79fc4",
   "metadata": {},
   "source": [
    "### Distribution of WER/CER rates for all datasets in the four quality bands, established via Levenshtein similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb6600",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define the bins and labels for quality bands\n",
    "# bins = [0, 0.7, 0.8, 0.9, 1]\n",
    "# labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['cer', 'wer']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Mean {error_rate.upper()}'] = dataset_data[[f'line-{error_rate}-ocr', \n",
    "#                                                  f'sentence-{error_rate}-ocr', \n",
    "#                                                  f'region-{error_rate}-ocr']].mean(axis=1)\n",
    "\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Mean {error_rate.upper()}\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "#         axs[i].set_xlim([0, 100])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         _ = fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f'Mean {error_rate.upper()} Error Rates across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define OCR noise level bins\n",
    "# # bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# # bins = [0, 0.7, 0.8, 0.9, 1.0]\n",
    "# bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# # Assign OCR noise level labels\n",
    "# # labels = [\"0-10%\", \"10-20%\", \"20-30%\", \"30-40%\", \"40-50%\", \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# # labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['lev']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "#                                                           f'sentence-{error_rate}-improvement', \n",
    "#                                                           f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Overall Levenshtein Improvement\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "# #         axs[i].set_ylim([0, 300])\n",
    "#         axs[i].set_xlim([-1, 1])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle('Overall Levenshtein Improvement across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Improvement Band'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079a585",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "print(n_figures)\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                      f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1, 1.2])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b6985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1.1, 1.1])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfe884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "        axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "        for i, prompt in enumerate(prompt_names):\n",
    "            prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "            \n",
    "            if 'icdar' not in dataset:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(prompt_data) > 0:\n",
    "                    sns.kdeplot(data=prompt_data, x=f'Overall Levenshtein Improvement', hue='model', \n",
    "                                fill=True, ax=axs[i], hue_order=MODELS)\n",
    "                    axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                    axs[i].set_xlim([-1, 1.2])\n",
    "                    axs[i].set_ylim([0, 1.4])\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "        # Remove empty subplot\n",
    "        fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d2e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd61e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        plt.figure(figsize=(10, 6))  # Adjust as necessary\n",
    "\n",
    "        for language in languages:\n",
    "            language_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt.isin(prompts_to_compare))]\n",
    "\n",
    "            if 'icdar' not in dataset:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(language_data) > 0:\n",
    "                    sns.kdeplot(data=language_data, x=f'Overall Levenshtein Improvement', hue='prompt', \n",
    "                                fill=True, hue_order=prompts_to_compare)\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {language} with {ex}')\n",
    "\n",
    "        plt.title(f'All Languages ({type_of_experiment})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.)\n",
    "\n",
    "\n",
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "bar_data = []\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for language in languages:\n",
    "            for prompt in prompts_to_compare:\n",
    "                sub_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt == prompt)]\n",
    "                if 'icdar' not in dataset:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                \n",
    "                mean_improvement = np.mean(sub_data[f'Overall Levenshtein Improvement'])\n",
    "                bar_data.append({'Language': language, 'Prompt': prompt, 'Mean Improvement': mean_improvement})\n",
    "\n",
    "bar_data = pd.DataFrame(bar_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Language', y='Mean Improvement', hue='Prompt', data=bar_data, hue_order=prompts_to_compare)\n",
    "plt.title(f'Mean Levenshtein Improvement for all Languages ({type_of_experiment})')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde994d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1)\n",
    "\n",
    "model_names = ['OPT', 'BLOOM', 'LLaMA', 'LLaMA 2', 'GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4']\n",
    "segmentations = ['line', 'sentence', 'region']\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for segmentation in segmentations:\n",
    "            fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "            axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "            for i, prompt in enumerate(prompt_names):\n",
    "                prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "                if segmentation == 'line':\n",
    "                    prompt_data = prompt_data[~prompt_data.dataset_name.isin(['icdar-2017', 'icdar-2019'])]\n",
    "                prompt_data[f'{segmentation.capitalize()} Levenshtein Improvement'] = prompt_data[f'{segmentation}-{error_rate}-improvement']\n",
    "\n",
    "                try:\n",
    "                    if len(prompt_data) > 0:\n",
    "                        sns.kdeplot(data=prompt_data, x=f'{segmentation.capitalize()} Levenshtein Improvement', hue='model', \n",
    "                                    fill=True, ax=axs[i], hue_order=model_names)\n",
    "                        axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                        axs[i].set_xlim([-1, 1.2])\n",
    "                        axs[i].set_ylim([0, 1.5])\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "            # Remove empty subplot\n",
    "            fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "            plt.suptitle(f'{segmentation.capitalize()} Levenshtein Improvement across Prompts and Models ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e54da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results = []  # Define results as a list\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA']\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for model in model_names:\n",
    "        model_data = data[(data.model == model) & (data.type == type_of_experiment)]\n",
    "        \n",
    "        # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "        if 'icdar' not in dataset:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'line-lev-improvement', \n",
    "                                                                        f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "        else:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "        # Append the results\n",
    "        print(model_data['Improvement Band'].unique())\n",
    "        results.append({'Model': model,\n",
    "                        'Type of Experiment': type_of_experiment,\n",
    "                        'Overall Levenshtein Improvement': np.nanmean(model_data['Overall Levenshtein Improvement'])})\n",
    "\n",
    "# Convert the results list to a DataFrame for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Type of Experiment', data=results_df, order=model_names)\n",
    "plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba62497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results = []  # Define results as a list\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA', 'LLaMA 2']\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for model in model_names:\n",
    "        for band in data['Improvement Band'].unique():\n",
    "            model_band_data = data[(data.model == model) & (data.type == type_of_experiment) & (data['Improvement Band'] == band)]\n",
    "            \n",
    "            # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "            if 'icdar' not in dataset:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'line-lev-improvement', \n",
    "                                                                                    f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "            # Append the results\n",
    "            results.append({'Model': model,\n",
    "                            'Type of Experiment': type_of_experiment,\n",
    "                            'Overall Levenshtein Improvement': np.nanmean(model_band_data['Overall Levenshtein Improvement']),\n",
    "                            'Improvement Band': band})\n",
    "\n",
    "# Convert the results list to a DataFrame for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Improvement Band', \n",
    "            data=results_df, order=model_names)\n",
    "plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA', 'LLaMA 2']\n",
    "improvement_bands = data['Improvement Band'].unique()\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    experiment_data = results_df[results_df['Type of Experiment'] == type_of_experiment]\n",
    "    \n",
    "    sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Improvement Band',\n",
    "                data=experiment_data, order=model_names, hue_order=improvement_bands)\n",
    "    \n",
    "    plt.title(f'Overall Levenshtein Improvement for {type_of_experiment} Models')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA', 'LLaMA 2']\n",
    "\n",
    "g = sns.FacetGrid(results_df, col='Type of Experiment', hue='Improvement Band', height=6, aspect=1)\n",
    "g.map(sns.barplot, 'Model', 'Overall Levenshtein Improvement', order=model_names)\n",
    "\n",
    "# Calculate means for each type of experiment and add horizontal lines\n",
    "for ax, (type_of_experiment, data) in zip(g.axes.flatten(), results_df.groupby('Type of Experiment')):\n",
    "    mean_improvement = data['Overall Levenshtein Improvement'].mean()\n",
    "    ax.axhline(mean_improvement, color='black', linestyle='--')\n",
    "    ax.text(0.6, mean_improvement, f'Mean: {mean_improvement:.2f}', color='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f708680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1)\n",
    "\n",
    "results = []  # Define results as a list\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA', 'LLaMA 2']\n",
    "level_names = ['line', 'sentence', 'region']\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for model in model_names:\n",
    "        model_data = data[(data['Model'] == model) & (data['Type of Experiment'] == type_of_experiment)]\n",
    "        \n",
    "        # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "        for level in level_names:\n",
    "            # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "            if 'icdar' not in dataset:\n",
    "                model_data['Overall Levenshtein Improvement'] = model_band_data[[f'line-lev-improvement', \n",
    "                                                                                    f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                model_data['Overall Levenshtein Improvement'] = model_band_data[[f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "            # Append the results\n",
    "            results.append({'Model': model,\n",
    "                            'Type of Experiment': type_of_experiment,\n",
    "                            'Level': level.capitalize(),\n",
    "                            'Levenshtein Improvement': np.nanmean(model_data['Overall Levenshtein Improvement'])})\n",
    "# Convert the results list to a DataFrame for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.catplot(x='Model', y='Levenshtein Improvement', hue='Type of Experiment', col='Level',\n",
    "            data=results_df, kind='bar', order=model_names)\n",
    "plt.suptitle('Levenshtein Improvement for Zero-Shot and Few-Shot Models', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd0c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.boxplot(x='Type of Experiment', y='Overall Levenshtein Improvement', hue='Model', \n",
    "            data=data, order=['zero-shot', 'few-shot'], hue_order=model_names)\n",
    "ax.set_ylim([-1, 1.2])\n",
    "ax.set_xticklabels(['Zero-Shot', 'Few-Shot'], rotation=0)\n",
    "ax.set_xlabel('Type of Experiment')\n",
    "ax.set_ylabel('Overall Levenshtein Improvement')\n",
    "plt.legend(title='Model', loc='upper right')\n",
    "\n",
    "plt.title('Overall Levenshtein Improvement across Type of Experiment and Models')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52706ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd342c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA']\n",
    "\n",
    "for prompt in data.prompt.unique():\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    dataset = data[data.prompt == prompt]\n",
    "    sns.boxplot(x='language', y='sentence-lev-improvement', hue='Model', hue_order=model_names,\n",
    "                data=dataset)\n",
    "    ax.set_ylim([-1, 1.2])\n",
    "    # ax.set_xticklabels(['Zero-Shot', 'Few-Shot'], rotation=0)\n",
    "    ax.set_xlabel('language')\n",
    "    ax.set_ylabel('sentence-lev-improvement')\n",
    "    plt.legend(title='Model', loc='upper right')\n",
    "\n",
    "    plt.title(f'Overall Levenshtein Improvement across Languages and Models ({prompt})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c23136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between quality band and imprevement band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68913b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818014ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_improvement_summary = data.groupby(['dataset_name', 'model'])[['line-lev-improvement', \n",
    "                                                                   'sentence-lev-improvement', \n",
    "                                                                   'region-lev-improvement']].mean().reset_index()\n",
    "# Compute the minimum and maximum Lev improvements for each dataset and model\n",
    "min_lev_improvements = data.groupby(['dataset_name', 'model'])[['line-lev-improvement', \n",
    "                                                                   'sentence-lev-improvement', \n",
    "                                                                   'region-lev-improvement']].min().unstack()\n",
    "max_lev_improvements = data.groupby(['dataset_name', 'model'])[['line-lev-improvement', \n",
    "                                                                   'sentence-lev-improvement', \n",
    "                                                                   'region-lev-improvement']].max().unstack()\n",
    "\n",
    "# Create a new column for the average improvement across all segment types\n",
    "lev_improvement_summary['Average Lev Improvement'] = lev_improvement_summary[['line-lev-improvement', 'sentence-lev-improvement', 'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "# Combine the mean, min, and max improvements into one DataFrame\n",
    "lev_improvements = pd.concat([lev_improvement_summary, min_lev_improvements, max_lev_improvements], \n",
    "                             keys=['Mean', 'Min', 'Max'])\n",
    "\n",
    "# Convert the DataFrame to a LaTeX table\n",
    "latex_table = lev_improvements.to_latex()\n",
    "\n",
    "print(latex_table)\n",
    "\n",
    "lev_improvement_summary.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faadfb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data so that models become columns and datasets become rows\n",
    "lev_improvement_pivot = lev_improvement_summary.pivot(index='dataset_name', columns='model', values='Average Lev Improvement')\n",
    "\n",
    "# Reset the index to make 'dataset_name' a regular column\n",
    "lev_improvement_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Convert the DataFrame to LaTeX and print it\n",
    "print(lev_improvement_pivot.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed503cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define OCR noise level bins\n",
    "# bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# bins = [0, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "# labels = [\"0-10%\", \"10-20%\", \"20-30%\", \"30-40%\", \"40-50%\", \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA']\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', 'prompt_complex_02',\n",
    "                'prompt_complex_lang']\n",
    "segment_types = ['line', 'sentence', 'region']\n",
    "\n",
    "for type_of_experiment in ['few-shot', 'zero-shot']:\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(n_datasets, len(segment_types), figsize=(20, 15))\n",
    "\n",
    "    for i, dataset in enumerate(dataset_names):\n",
    "        dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "\n",
    "        for j, segment_type in enumerate(segment_types):\n",
    "            # Compute the segment type lev improvement\n",
    "            dataset_data[f'{segment_type.capitalize()} Levenshtein Improvement'] = dataset_data[f'{segment_type}-{error_rate}-improvement']\n",
    "            \n",
    "            try:\n",
    "                if type_of_experiment == 'language-specific':\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'{segment_type.capitalize()} Levenshtein Improvement', data=dataset_data, \n",
    "                                ax=axs[i, j], order=model_names, hue='prompt')\n",
    "                else:\n",
    "                    _ = sns.boxplot(x='model', y=f'{segment_type.capitalize()} Levenshtein Improvement', data=dataset_data, \n",
    "                                ax=axs[i, j], order=model_names, hue='prompt', hue_order=prompt_names)\n",
    "            except Exception as ex:\n",
    "                print(f'Could not load {dataset} with {ex}')\n",
    "            axs[i, j].set_title(f'{dataset.upper()} - {segment_type.capitalize()}')\n",
    "#             axs[i, j].set_xticklabels(axs[i, j].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "            axs[i, j].set_xlabel('')  # Remove x-axis label\n",
    "            axs[i, j].set_ylabel('')  # Remove y-axis label\n",
    "\n",
    "            axs[i, j].set_ylim([-1.2, 1.2])\n",
    "            # Remove the legend for all but the last subplot\n",
    "            if i < n_datasets - 1 or j < len(segment_types):\n",
    "                axs[i, j].legend().set_visible(False)\n",
    "\n",
    "\n",
    "    # Add a legend outside of the plot area of the last subplot\n",
    "    handles, labels = axs[-1, -1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.03), ncol=len(handles))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08)  # Adjust the overall layout\n",
    "    plt.suptitle(f'Levenshtein Improvement across Datasets, Models, Prompts, and Segment Types ({type_of_experiment})', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of quality bands\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "\n",
    "for is_few_shot in [False]:\n",
    "    # Create a figure and axes for subplots\n",
    "    fig, axs = plt.subplots(len(quality_bands), len(dataset_names), figsize=(20, 15))\n",
    "\n",
    "    # Flatten the axes for easy iteration\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Create a counter\n",
    "    i = 0\n",
    "    # Iterate over all datasets\n",
    "    for dataset in dataset_names:\n",
    "        dataset_data = data[(data.dataset_name == dataset) & (data.type == is_few_shot)]\n",
    "\n",
    "        # Iterate over all quality bands\n",
    "        for band in quality_bands:\n",
    "            # Filter data based on dataset and quality band\n",
    "            subset = dataset_data[dataset_data['Quality Band'] == band]\n",
    "\n",
    "            # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "            subset[f'Overall Levenshtein Improvement'] = subset[[f'line-lev-improvement', \n",
    "                                                              f'sentence-lev-improvement', \n",
    "                                                              f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "            # Plot the distribution of improvements for each model\n",
    "            _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=subset, \n",
    "                        ax=axs[i], order=model_names, hue='prompt', hue_order=prompt_names)\n",
    "\n",
    "            axs[i].set_title(f'{dataset.upper()} \\n Quality Band {band}')\n",
    "\n",
    "            axs[i].set_ylim([-1, 1.2])\n",
    "            axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "            axs[i].set_xlabel('')  # Remove x-axis label\n",
    "            axs[i].set_ylabel('')  # Remove y-axis label\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    # Remove empty subplots\n",
    "    for j in range(i, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if is_few_shot:\n",
    "        plt.suptitle('Overall Levenshtein Improvement across Datasets, Models, Prompts, and Quality Bands (few-shot)', fontsize=20, y=1.02)\n",
    "    else:\n",
    "        plt.suptitle('Overall Levenshtein Improvement across Datasets, Models, Prompts, and Quality Bands', fontsize=20, y=1.02)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ed1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a mapping from quality bands to numeric values\n",
    "quality_band_mapping = {\"0-40%\": 1, \"40-60%\": 2, \"60-80%\": 3, \"80-99%\": 4, \"99-100%\": 5}\n",
    "\n",
    "# Create a numeric quality band column based on the defined bins and labels\n",
    "data[\"Quality Band\"] = pd.cut(data['line-lev-ocr'], bins=[0, 0.4, 0.6, 0.8, 0.99, 1], labels=quality_band_mapping.keys())\n",
    "\n",
    "# Add a numeric quality band column to the data\n",
    "data[\"Numeric Quality Band\"] = data[\"Quality Band\"].map(quality_band_mapping)\n",
    "\n",
    "# Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "data['Overall Levenshtein Improvement'] = data[[f'line-lev-improvement', \n",
    "                                              f'sentence-lev-improvement', \n",
    "                                              f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Generate scatter plot\n",
    "sns.scatterplot(data=data, x='Numeric Quality Band', y='Overall Levenshtein Improvement', hue='model')\n",
    "\n",
    "# Improve readability\n",
    "plt.xticks(ticks=range(1, 6), labels=quality_band_mapping.keys())\n",
    "plt.xlabel('Quality Band')\n",
    "plt.ylabel('Overall Levenshtein Improvement')\n",
    "plt.title('Levenshtein Improvement vs Quality Band')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "data['Overall Levenshtein Improvement'] = data[[f'line-lev-improvement', \n",
    "                                              f'sentence-lev-improvement', \n",
    "                                              f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate violin plot\n",
    "sns.violinplot(data=data, x='Quality Band', y='Overall Levenshtein Improvement', hue='dataset_name')\n",
    "\n",
    "# Improve readability\n",
    "plt.xlabel('Quality Band')\n",
    "plt.ylabel('Overall Levenshtein Improvement')\n",
    "plt.title('Levenshtein Improvement vs Quality Band for each Dataset')\n",
    "plt.legend(title='Dataset', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5b4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lev_improvement_threshold = -1.0\n",
    "\n",
    "dataset = 'ajmc-mixed'\n",
    "\n",
    "for is_few_shot in [True, False]:\n",
    "    # Create subplots\n",
    "\n",
    "    dataset_data = data[(data.dataset_name == dataset) & (data.type == is_few_shot)]\n",
    "    segment_type = 'sentence'\n",
    "    for error_rate in ['lev']:\n",
    "        interesting_cases = dataset_data[dataset_data[f'{segment_type}-{error_rate}-improvement'] > lev_improvement_threshold]\n",
    "\n",
    "        for _, interesting_cases in interesting_cases.iterrows():\n",
    "            print('Segment type:', segment_type)\n",
    "            print('Model:', interesting_cases['model'])\n",
    "            print('Dataset:', interesting_cases['dataset_name'])\n",
    "            print('Quality Band:', interesting_cases[f'Quality Band'])\n",
    "\n",
    "            print('LEV ground-ocr', interesting_cases[f'{segment_type}-lev-ocr'], \n",
    "                  'LEV ground-pred', interesting_cases[f'{segment_type}-lev-pred'])\n",
    "            print('LEV Improvement:', interesting_cases[f'{segment_type}-lev-improvement'])\n",
    "\n",
    "            print('Ground:', interesting_cases[f'groundtruth.{segment_type}'][:50])\n",
    "            print('OCR:', interesting_cases[f'ocr.{segment_type}'][:50])\n",
    "            print('Pred:', interesting_cases[f'prediction.{segment_type}'])\n",
    "            print('--'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6eea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[f'sentence-lev-improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.model.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5750bc",
   "metadata": {},
   "source": [
    "### ICDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac3e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_2017 = data[data.dataset_name == 'icdar-2017']\n",
    "\n",
    "def get_document_type(filename):\n",
    "    document_type = filename.split('/')[-2].replace('_', '-')\n",
    "    return document_type\n",
    "\n",
    "icdar_2017['document_type'] = icdar_2017['filename'].apply(lambda x: get_document_type(x))\n",
    "\n",
    "icdar_2017.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the first dataset name\n",
    "dataset_name = 'ICDAR-2017'\n",
    "\n",
    "dataset_data = icdar_2017\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA']\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01']\n",
    "segment_types = ['sentence', 'region']\n",
    "document_types = dataset_data['document_type'].unique()  # Get the unique document types\n",
    "\n",
    "for error_rate in ['lev']:\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(len(segment_types), len(document_types), figsize=(20, 15))\n",
    "\n",
    "    for i, segment_type in enumerate(segment_types):\n",
    "        for j, document_type in enumerate(document_types):\n",
    "            # Select data for the current document type\n",
    "            document_data = dataset_data[dataset_data.document_type == document_type]\n",
    "\n",
    "            # Compute the segment type lev improvement\n",
    "            document_data[f'{segment_type.capitalize()} Levenshtein Improvement'] = document_data[f'{segment_type}-{error_rate}-improvement']\n",
    "\n",
    "            # Plot the distribution of improvements for each model\n",
    "            _ = sns.boxplot(x='model', y=f'{segment_type.capitalize()} Levenshtein Improvement', data=document_data, \n",
    "                        ax=axs[i, j], order=model_names, hue='prompt', hue_order=prompt_names)\n",
    "\n",
    "            axs[i, j].set_title(f'{dataset_name.upper()} - {segment_type.capitalize()} - {document_type}')\n",
    "            axs[i, j].set_xticklabels(axs[i, j].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "            axs[i, j].set_xlabel('')  # Remove x-axis label\n",
    "            axs[i, j].set_ylabel('')  # Remove y-axis label\n",
    "            axs[i, j].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "            # Remove the legend for all but the last subplot\n",
    "            if i < len(segment_types) - 1 or j < len(document_types) - 1:\n",
    "                axs[i, j].legend().set_visible(False)\n",
    "\n",
    "    # Add a legend outside of the plot area of the last subplot\n",
    "    handles, labels = axs[-1, -1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.03), ncol=len(handles))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08)  # Adjust the overall layout\n",
    "    plt.suptitle('Levenshtein Improvement across Document Types, Models, Prompts, and Segment Types', fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522b5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "icdar_2019 = data[data.dataset_name == 'icdar-2019']\n",
    "\n",
    "def get_document_type(filename):\n",
    "    document_type = filename.split('/')[-3]\n",
    "    return document_type\n",
    "\n",
    "icdar_2019['document_type'] = icdar_2019['filename'].apply(lambda x: get_document_type(x))\n",
    "\n",
    "icdar_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the first dataset name\n",
    "dataset_name = 'ICDAR-2019'\n",
    "\n",
    "dataset_data = icdar_2019\n",
    "\n",
    "model_names = ['GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4', 'OPT', 'BLOOM', 'LLaMA']\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01']\n",
    "segment_types = ['sentence', 'region']\n",
    "document_types = dataset_data['document_type'].unique()  # Get the unique document types\n",
    "\n",
    "n_rows = -(-len(segment_types) * len(document_types) // 4)  # Calculate the number of rows needed\n",
    "\n",
    "for error_rate in ['lev']:\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(n_rows, 4, figsize=(20, 15))\n",
    "    axs = axs.flatten()  # Flatten the axes for easy iteration\n",
    "\n",
    "    for i, (segment_type, document_type) in enumerate([(st, dt) for st in segment_types for dt in document_types]):\n",
    "        # Select data for the current document type\n",
    "        document_data = dataset_data[dataset_data.document_type == document_type]\n",
    "\n",
    "        # Compute the segment type lev improvement\n",
    "        document_data[f'{segment_type.capitalize()} Levenshtein Improvement'] = document_data[f'{segment_type}-{error_rate}-improvement']\n",
    "\n",
    "        # Plot the distribution of improvements for each model\n",
    "        _ = sns.boxplot(x='model', y=f'{segment_type.capitalize()} Levenshtein Improvement', data=document_data, \n",
    "                    ax=axs[i], order=model_names, hue='prompt', hue_order=prompt_names)\n",
    "\n",
    "        axs[i].set_title(f'{dataset_name.upper()} - {segment_type.capitalize()} - {document_type}')\n",
    "        axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "        axs[i].set_xlabel('')  # Remove x-axis label\n",
    "        axs[i].set_ylabel('')  # Remove y-axis label\n",
    "        axs[i].set_ylim([-1, 1])  # Set y-axis limits\n",
    "\n",
    "        # Remove the legend for all but the last subplot\n",
    "        if i < len(axs) - 1:\n",
    "            axs[i].legend().set_visible(False)\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for i in range(len(segment_types) * len(document_types), len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    # Add a legend outside of the plot area of the last subplot\n",
    "    handles, labels = axs[-1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.03), ncol=len(handles))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08)  # Adjust the overall layout\n",
    "    plt.suptitle('Levenshtein Improvement across Document Types, Models, Prompts, and Segment Types', fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc6781",
   "metadata": {},
   "source": [
    "### AJMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ajmc_mixed = data[data.dataset_name == 'ajmc-mixed']\n",
    "\n",
    "def get_document_type(filename):\n",
    "    print(filename)\n",
    "#     document_type = filename.split('/')[-3]\n",
    "    return filename\n",
    "\n",
    "ajmc_mixed['document_type'] = ajmc_mixed['filename'].apply(lambda x: get_document_type(x))\n",
    "\n",
    "ajmc_mixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ajmc_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dca92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56750d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90ec90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afacc49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, result in enumerate(results):\n",
    "for segment_type in ['line', 'sentence', 'region']:\n",
    "    print(segment_type, 'level', '*'*50)\n",
    "    try:\n",
    "        improved_texts = dataset[dataset['model'] == 'GPT-4'] \n",
    "        impooved_texts = improved_texts[improved_texts[f'{segment_type}-lev-improvement'] >= 0.7]\n",
    "        #result[result[f'{segment_type}-lev-improvement'] >= 0.3]\n",
    "        for _, improved_text in improved_texts.iterrows():\n",
    "            print('Model:', improved_texts['model'].unique()[0])\n",
    "            print('Dataset:', improved_texts['dataset_name'].unique()[0])\n",
    "            print('Quality Band:', improved_text[f'{segment_type}-ocr-noise-group'])\n",
    "\n",
    "            print('LEV ground-ocr', improved_text[f'{segment_type}-lev-ocr'], \n",
    "                  'LEV ground-pred', improved_text[f'{segment_type}-lev-pred'])\n",
    "            print('LEV Improvement:', improved_text[f'{segment_type}-lev-improvement'])\n",
    "            print('CER Improvement:', improved_text[f'{segment_type}-cer-improvement'])\n",
    "            print('WER Improvement:', improved_text[f'{segment_type}-wer-improvement'])\n",
    "\n",
    "            print('Ground:', improved_text[f'groundtruth.{segment_type}'][:50])\n",
    "            print('OCR:', improved_text[f'ocr.{segment_type}'][:50])\n",
    "            print('Pred:', improved_text[f'prediction.{segment_type}'])\n",
    "            print('--'*50)\n",
    "    except:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a3773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c47988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e208ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258b74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0143228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded1db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2da865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f28ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f379f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d3027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set the colorblind color palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for model in data['model'].unique():\n",
    "    data_per_model = data[data['model'] == model]\n",
    "    for segment_type in ['line', 'sentence', 'region']:\n",
    "        # Filter the data by dataset and segment type\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Create the box plot\n",
    "        sns.boxplot(x=f'{segment_type}-ocr-noise-group', y=f'{segment_type}-lev-improvement', \n",
    "                    data=data_per_model, hue='dataset_name', palette='colorblind', ax=ax)\n",
    "\n",
    "        # Set the plot title and axis labels\n",
    "        plt.title(f'Levenshtein Distance Improvement for {segment_type.capitalize()} Segments ({model})')\n",
    "        plt.xlabel('Quality Bands')\n",
    "        plt.ylabel('Levenshtein Distance Improvement')\n",
    "        \n",
    "        plt.ylim((-0.5, 1.5))\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e4203",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in data['dataset_name'].unique():\n",
    "    data_per_model = data[data['dataset_name'] == model]\n",
    "    for segment_type in ['line', 'sentence', 'region']:\n",
    "        # Filter the data by dataset and segment type\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        # Create the box plot\n",
    "        sns.boxplot(x=f'{segment_type}-ocr-noise-group', y=f'{segment_type}-lev-improvement', \n",
    "                    data=data_per_model, hue='model', palette='colorblind', ax=ax)\n",
    "\n",
    "        # Set the plot title and axis labels\n",
    "        plt.title(f'Levenshtein Distance Improvement for {segment_type.capitalize()} Segments ({model})')\n",
    "        plt.xlabel('Quality Bands')\n",
    "        plt.ylabel('Levenshtein Distance Improvement')\n",
    "        \n",
    "        plt.ylim((-0.2, 1.0))\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902d03b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fe33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set the colorblind color palette\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for result in results:\n",
    "    \n",
    "    dataset_name = result['dataset_name'].unique()[0]\n",
    "    \n",
    "    for segment_type in ['line', 'sentence', 'region']:\n",
    "        \n",
    "        #grouped_results = result.groupby([f\"{segment_type}-ocr-noise-group\", \"dataset_name\"]).size().reset_index(name=\"count\")\n",
    "        \n",
    "        grouped_results = result.groupby([f\"{segment_type}-ocr-noise-group\", \"dataset_name\"])[f\"{segment_type}-lev-improvement\"].mean().reset_index()\n",
    "\n",
    "        print(grouped_results.head())\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        _ = sns.lineplot(x=f\"{segment_type}-ocr-noise-group\", y=f'{segment_type}-lev-improvement', hue='dataset_name',\n",
    "                 data=grouped_results, ax=ax, markers=True, linestyle='-', linewidth=2.5)\n",
    "\n",
    "\n",
    "        # Set plot labels\n",
    "        ax.set_xlabel(f\"{dataset_name} Ground Truth {segment_type.capitalize()}\")\n",
    "        ax.set_ylabel(f\"{dataset_name} {segment_type.capitalize()} Improvement\")\n",
    "        ax.set_title(f\"{dataset_name} Levenshtein Improvement for {segment_type.capitalize()} OCR Examples\")\n",
    "\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b6097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff5018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d142c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
