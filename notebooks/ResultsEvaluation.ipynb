{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfadbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "\n",
    "import os\n",
    "import json\n",
    "# !pip install pywer\n",
    "import pywer\n",
    "# !pip install pyjarowinkler\n",
    "from pyjarowinkler import distance as jwdistance\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Const:\n",
    "    OCR = 'ocr'\n",
    "    GROUND = 'groundtruth'\n",
    "    REGION = 'region'\n",
    "    LINE = 'line'\n",
    "    SENTENCE = 'sentence'\n",
    "    FILE = 'filename'\n",
    "    DATASET = 'dataset_name'\n",
    "    PREDICTION = 'prediction'\n",
    "    PROMPT = 'prompt'\n",
    "    LANGUAGE = 'language'\n",
    "    NONE = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ead6e",
   "metadata": {},
   "source": [
    "### Lookup datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79f14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/datasets/ocr/converted/ajmc-mixed/ajmc_mixed.jsonl ajmc-mixed\n",
      "../data/datasets/ocr/converted/overproof/overproof.jsonl overproof\n",
      "../data/datasets/ocr/converted/icdar-2019/icdar-2019.jsonl icdar-2019\n",
      "../data/datasets/ocr/converted/icdar-2017/icdar-2017.jsonl icdar-2017\n",
      "../data/datasets/ocr/converted/impresso/impresso-nzz.jsonl impresso\n",
      "../data/datasets/ocr/converted/ajmc-primary/ajmc_primary_text.jsonl ajmc-primary\n",
      "../data/datasets/htr/converted/htrec/htrec.jsonl htrec\n",
      "../data/datasets/asr/converted/ina/ina.jsonl ina\n"
     ]
    }
   ],
   "source": [
    "datasets = []\n",
    "\n",
    "for type_document in ['ocr', 'htr', 'asr']:\n",
    "    for root, dirs, files in os.walk(f'../data/datasets/{type_document}/converted'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                if 'sample' not in input_file:\n",
    "                    with open(input_file) as f:\n",
    "                        lines = f.read().splitlines()\n",
    "                    df_inter = pd.DataFrame(lines)\n",
    "                    df_inter.columns = ['json_element']\n",
    "                    df_inter['json_element'].apply(json.loads)\n",
    "                    df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    print(input_file, dataset_name)\n",
    "                    df['dataset_name'] = [dataset_name] * len(df)\n",
    "                    if 'ajmc' in dataset_name:\n",
    "                        df['language'] = ['el'] * len(df)\n",
    "                    if 'overproof' in dataset_name:\n",
    "                        df['language'] = ['en'] * len(df)\n",
    "                    if 'impresso' in dataset_name:\n",
    "                        df['language'] = ['de'] * len(df)\n",
    "\n",
    "                    datasets.append(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58611ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lines/sentences/regions.\n",
      "\n",
      "Dataset: ajmc-mixed 1291 with duplicates\n",
      "No. lines: 535 / 1291 No. sentences: 379 / 1291 No. regions: 33 / 1291\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: overproof 2669 with duplicates\n",
      "No. lines: 2278 / 2669 No. sentences: 399 / 2669 No. regions: 41 / 2669\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: icdar-2019 404 with duplicates\n",
      "No. lines: 0 / 404 No. sentences: 404 / 404 No. regions: 41 / 404\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: icdar-2017 477 with duplicates\n",
      "No. lines: 0 / 477 No. sentences: 461 / 477 No. regions: 28 / 477\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: impresso 1563 with duplicates\n",
      "No. lines: 1256 / 1563 No. sentences: 577 / 1563 No. regions: 203 / 1563\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ajmc-primary 57 with duplicates\n",
      "No. lines: 40 / 57 No. sentences: 27 / 57 No. regions: 9 / 57\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: htrec 180 with duplicates\n",
      "No. lines: 180 / 180 No. sentences: 8 / 180 No. regions: 8 / 180\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset: ina 489 with duplicates\n",
      "No. lines: 201 / 489 No. sentences: 290 / 489 No. regions: 6 / 489\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique lines/sentences/regions.\\n')\n",
    "for dataset in datasets:\n",
    "    print('Dataset:', dataset['dataset_name'].unique()[0], len(dataset), 'with duplicates')\n",
    "    print('No. lines:', dataset['ocr.line']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. sentences:', dataset['ocr.sentence']. nunique(), '/', len(dataset['ocr.sentence']), \n",
    "          'No. regions:', dataset['ocr.region']. nunique(), '/', len(dataset['ocr.region']))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd830ab",
   "metadata": {},
   "source": [
    "## Step 1: Loading of preliminary results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b00aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_line_by_line(filename):\n",
    "    pass\n",
    "\n",
    "def json_load(text):\n",
    "    \n",
    "    try:\n",
    "        loaded_line = json.loads(text)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(text[:30], '...')\n",
    "        loaded_line = 'No text'\n",
    "    return loaded_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1666da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icdar_2017_lang(filename):\n",
    "    lang = filename.split('/')[-2].split('_')[0]\n",
    "    if lang =='eng':\n",
    "        lang = 'en'\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8200754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cff1950d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                                                                                                              | 568/1050 [00:46<00:21, 22.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra data: line 1 column 15130 (char 15129)\n",
      "{\"language\": \"icdar2017_datase ...\n",
      "We could not load ../data/output/few_shot/prompt_complex_02/icdar-2017/results-3few-shot-icdar-2017-decapoda-research-llama-7b-hf.jsonl Extra data: line 1 column 15130 (char 15129)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1050/1050 [01:21<00:00, 12.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "results = []\n",
    "\n",
    "# First traversal to get count of .jsonl files\n",
    "jsonl_count = 0\n",
    "for root, dirs, files in os.walk('../data/output'):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jsonl\"):\n",
    "            jsonl_count += 1\n",
    "\n",
    "# Second traversal to do the processing with tqdm progress bar\n",
    "with tqdm(total=jsonl_count) as pbar:\n",
    "    for root, dirs, files in os.walk('../data/output'):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jsonl\"):\n",
    "                input_file = os.path.join(root, file)\n",
    "                is_few = False\n",
    "                if 'few' in input_file:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    is_few = True\n",
    "                else:\n",
    "                    prompt = root.split('/')[-2]\n",
    "                    \n",
    "                if 'sample' not in input_file:\n",
    "#                     if 'prompt_complex_lang' in input_file:\n",
    "#                         print(input_file)\n",
    "#                     prompt = root.split('/')[-2]\n",
    "                    try:\n",
    "                        with open(input_file) as f:\n",
    "                            text = f.read()\n",
    "                    \n",
    "                        with open(input_file) as f:\n",
    "                            lines = f.readlines()\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                        continue\n",
    "                    # Check correct lines\n",
    "                    text = text.replace('\\n', '')\n",
    "                    text_list = text.split('}}{\"')\n",
    "                    json_objects = []\n",
    "\n",
    "                    for i, t in enumerate(text_list):\n",
    "                        if i != 0:\n",
    "                            t = '{\"' + t\n",
    "                        if i != len(text_list) - 1:\n",
    "                            t = t + '}'\n",
    "                        if not t.endswith('}}'):\n",
    "                            json_objects.append(t + '}\\n')\n",
    "                        else:\n",
    "                            json_objects.append(t + '\\n')\n",
    "                        \n",
    "                    df_inter = pd.DataFrame(json_objects)\n",
    "                    df_inter.columns = ['json_element']\n",
    "\n",
    "                    dataset_name = root.split('/')[-1].replace('_', '-')\n",
    "                    model_dataset_name = file[8:-6]\n",
    "                    model_name = model_dataset_name.replace(root.split('/')[-1] + '-', '').strip()\n",
    "                    try:\n",
    "                        df_inter['json_element'].apply(lambda x: json_load(x))\n",
    "                        df = pd.json_normalize(df_inter['json_element'].apply(json.loads))\n",
    "\n",
    "                        df['model'] = [model_name] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        df['prompt'] = [prompt] * len(df)\n",
    "                        try:\n",
    "                            with open(f'../data/prompts/{prompt}.txt', 'r') as f:\n",
    "                                prompt_text = f.read()\n",
    "                        except:\n",
    "                            prompt_text = 'prompt_complex_03_per_lang'\n",
    "                            \n",
    "                        df['prompt_text'] = [prompt_text] * len(df)\n",
    "                        \n",
    "                        if is_few:\n",
    "                            df['type'] = ['few-shot'] * len(df)\n",
    "                        else:\n",
    "                            df['type'] = ['zero-shot'] * len(df)\n",
    "\n",
    "                        df['dataset_name'] = [dataset_name] * len(df)\n",
    "                        \n",
    "                        if 'ajmc' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "                        if 'ina' in dataset_name:\n",
    "                            df['language'] = ['fr'] * len(df)\n",
    "                        if 'overproof' in dataset_name:\n",
    "                            df['language'] = ['en'] * len(df)\n",
    "                        if 'impresso' in dataset_name:\n",
    "                            df['language'] = ['de'] * len(df)\n",
    "                        if 'htrec' in dataset_name:\n",
    "                            df['language'] = ['el'] * len(df)\n",
    "\n",
    "#                         print(dataset_name, model_name, prompt)\n",
    "\n",
    "                        if dataset_name == 'icdar-2017':\n",
    "                            df['language'] = df['filename'].apply(get_icdar_2017_lang)\n",
    "                        \n",
    "                        results.append(df)\n",
    "#                         print(df.prompt.unique())\n",
    "                    except Exception as ex:\n",
    "                        print('We could not load {} {}'.format(input_file, ex))\n",
    "                    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223ddaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea001bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>ocr.line</th>\n",
       "      <th>ocr.sentence</th>\n",
       "      <th>ocr.region</th>\n",
       "      <th>groundtruth.line</th>\n",
       "      <th>groundtruth.sentence</th>\n",
       "      <th>groundtruth.region</th>\n",
       "      <th>prediction.prompt</th>\n",
       "      <th>prediction.line</th>\n",
       "      <th>prediction.sentence</th>\n",
       "      <th>prediction.region</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>type</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/datasets/ocr/original/impresso-nzz/...</td>\n",
       "      <td>impresso-nzz</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>bigscience-bloomz-560m</td>\n",
       "      <td>prompt_basic_02</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/datasets/ocr/original/impresso-nzz/...</td>\n",
       "      <td>impresso-nzz</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>Pr√§numeration.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>Pr√§numeration.&lt;/s&gt;</td>\n",
       "      <td>bigscience-bloomz-560m</td>\n",
       "      <td>prompt_basic_02</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/datasets/ocr/original/impresso-nzz/...</td>\n",
       "      <td>impresso-nzz</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich.............</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich.............</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>J√§hrlich........... Hah&lt;/s&gt;</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>bigscience-bloomz-560m</td>\n",
       "      <td>prompt_basic_02</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/datasets/ocr/original/impresso-nzz/...</td>\n",
       "      <td>impresso-nzz</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich.............</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich.............</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>J√§hrlich........... Hah&lt;/s&gt;</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>bigscience-bloomz-560m</td>\n",
       "      <td>prompt_basic_02</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/datasets/ocr/original/impresso-nzz/...</td>\n",
       "      <td>impresso-nzz</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>8 Fr.</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>8 Fr.</td>\n",
       "      <td>J√§hrlich............. 8 Fr.</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>4 Fr. 5 Fr. 6 Fr. 7 Fr. 8</td>\n",
       "      <td>/s&gt;</td>\n",
       "      <td>bigscience-bloomz-560m</td>\n",
       "      <td>prompt_basic_02</td>\n",
       "      <td>Correct the spelling and grammar of the follow...</td>\n",
       "      <td>zero-shot</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  dataset_name   \n",
       "0  ../../data/datasets/ocr/original/impresso-nzz/...  impresso-nzz  \\\n",
       "1  ../../data/datasets/ocr/original/impresso-nzz/...  impresso-nzz   \n",
       "2  ../../data/datasets/ocr/original/impresso-nzz/...  impresso-nzz   \n",
       "3  ../../data/datasets/ocr/original/impresso-nzz/...  impresso-nzz   \n",
       "4  ../../data/datasets/ocr/original/impresso-nzz/...  impresso-nzz   \n",
       "\n",
       "                      ocr.line           ocr.sentence   \n",
       "0               Pr√§numeration.         Pr√§numeration.  \\\n",
       "1               Pr√§numeration.         Pr√§numeration.   \n",
       "2  J√§hrlich............. 8 Fr.  J√§hrlich.............   \n",
       "3  J√§hrlich............. 8 Fr.  J√§hrlich.............   \n",
       "4  J√§hrlich............. 8 Fr.                  8 Fr.   \n",
       "\n",
       "                    ocr.region             groundtruth.line   \n",
       "0               Pr√§numeration.               Pr√§numeration.  \\\n",
       "1               Pr√§numeration.               Pr√§numeration.   \n",
       "2  J√§hrlich............. 8 Fr.  J√§hrlich............. 8 Fr.   \n",
       "3  J√§hrlich............. 8 Fr.  J√§hrlich............. 8 Fr.   \n",
       "4  J√§hrlich............. 8 Fr.  J√§hrlich............. 8 Fr.   \n",
       "\n",
       "    groundtruth.sentence           groundtruth.region   \n",
       "0         Pr√§numeration.               Pr√§numeration.  \\\n",
       "1         Pr√§numeration.               Pr√§numeration.   \n",
       "2  J√§hrlich.............  J√§hrlich............. 8 Fr.   \n",
       "3  J√§hrlich.............  J√§hrlich............. 8 Fr.   \n",
       "4                  8 Fr.  J√§hrlich............. 8 Fr.   \n",
       "\n",
       "                                   prediction.prompt     prediction.line   \n",
       "0  Correct the spelling and grammar of the follow...  Pr√§numeration.</s>  \\\n",
       "1                                                NaN  Pr√§numeration.</s>   \n",
       "2  Correct the spelling and grammar of the follow...                 /s>   \n",
       "3                                                NaN                 /s>   \n",
       "4  Correct the spelling and grammar of the follow...                 /s>   \n",
       "\n",
       "           prediction.sentence   prediction.region                   model   \n",
       "0           Pr√§numeration.</s>  Pr√§numeration.</s>  bigscience-bloomz-560m  \\\n",
       "1           Pr√§numeration.</s>  Pr√§numeration.</s>  bigscience-bloomz-560m   \n",
       "2  J√§hrlich........... Hah</s>                 /s>  bigscience-bloomz-560m   \n",
       "3  J√§hrlich........... Hah</s>                 /s>  bigscience-bloomz-560m   \n",
       "4    4 Fr. 5 Fr. 6 Fr. 7 Fr. 8                 /s>  bigscience-bloomz-560m   \n",
       "\n",
       "            prompt                                        prompt_text   \n",
       "0  prompt_basic_02  Correct the spelling and grammar of the follow...  \\\n",
       "1  prompt_basic_02  Correct the spelling and grammar of the follow...   \n",
       "2  prompt_basic_02  Correct the spelling and grammar of the follow...   \n",
       "3  prompt_basic_02  Correct the spelling and grammar of the follow...   \n",
       "4  prompt_basic_02  Correct the spelling and grammar of the follow...   \n",
       "\n",
       "        type language  \n",
       "0  zero-shot       de  \n",
       "1  zero-shot       de  \n",
       "2  zero-shot       de  \n",
       "3  zero-shot       de  \n",
       "4  zero-shot       de  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08ab9f",
   "metadata": {},
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d50ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "from Levenshtein import distance\n",
    "\n",
    "def compute_normalized_levenshtein_similarity(ground_truth_text, ocr_text):\n",
    "    length = max(len(ocr_text), len(ground_truth_text))\n",
    "    levenshtein_distance = distance(ocr_text, ground_truth_text)\n",
    "    similarity = (length - levenshtein_distance) / length\n",
    "    return similarity\n",
    "\n",
    "def compute_jaccard(ocr_text, ground_truth_text):\n",
    "    try: \n",
    "        return jwdistance.get_jaro_distance(ocr_text, ground_truth_text)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def get_improvement(original_similarity, corrected_similarity):\n",
    "    \n",
    "    if original_similarity == 0:\n",
    "        return min(max(corrected_similarity, -1), 1)\n",
    "    elif original_similarity != corrected_similarity:\n",
    "        return min(max((corrected_similarity - original_similarity) / original_similarity, -1), 1)\n",
    "    elif original_similarity == corrected_similarity:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0 if corrected_similarity < 1 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f8de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9839406962029382"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = \"149 Obr√°zek z malomƒõstsk√©ho kuk√°tka. Pod√°v√° L. Gro\"\n",
    "ocr_text = \"149 Obr√°zek z malomƒõstsk√©ho kuk√°tka. Pod√°v√° L. Grw\"\n",
    "pred_text = \"\"\"Correct the text: \"149 Obr√°zek z malomƒõstsk√©ho kuk√°tka. Pod√°v√° L. Grwsmannov√°-Brodsk√°. M√≠sto dƒõje: salon pan√≠ stavitelky; doba: ƒças vƒõnovan√Ω k√°vov√© ‚Äûvisitƒõ\"; jednaj√≠c√≠ osoby: d√°my p≈ôednƒõj≈°√≠ honorace mƒõstsk√©, z nich≈æ ‚Ä¢vƒõt≈°ina je mlad√° a nƒõkter√© skuteƒçnƒõ hezk√©. Na stole prost≈ôen√©m kr√°sn√Ωm ubrusem dama≈°kov√Ωm stoj√≠ tal√≠≈ôe s kol√°ƒçky, vƒõneƒçky a precl√≠ƒçky, kol toho pƒõknƒõ se vyj√≠maj√≠ k≈ôi≈°≈•√°lov√© sklenky s vodou, st≈ô√≠brn√© l≈æiƒçky a ≈°√°lky z jemn√©ho porcel√°nu, jejich≈æ vonn√Ω obsah na p≈ô√≠tomn√© paniƒçky zd√° se velmi blaze p≈Øsobiti. Ze ≈æivƒõj≈°√≠ho hovoru vyzn√≠v√° pr√°vƒõ hlas pan√≠ not√°≈ôky, kter√° ve spravedliv√©m rozhorlen√≠ mluv√≠: ‚ÄûAno, m√© d√°my, ji≈æ jest to takov√©! Samy r√°ƒçily jste b√Ωti svƒõdky, jak svornƒõ i jednohlasnƒõ byl p≈ôijat n√°vrh pan√≠ sl√°dkov√©, abychom si po≈ô√≠dily kroje n√°rodn√≠ a tak p≈ôispƒõly ku zv√Ω≈°en√≠ lesku slavnosti, ji≈æ po≈ô√°d√° n√°≈° stateƒçn√Ω studentsk√Ω spolek ‚ÄûHvƒõzda\", a kdy≈æ ji≈æ n√°s p√°ni akademikov√© poctili d≈Øvƒõrou, ≈æe v na≈°e ruce slo≈æili starost o buffet a jin√© je≈°tƒõ funkce, to≈æ mƒõly bychom snad t√©≈æ jiti za p≈ô√≠kladem sleƒçen bern√≠ch a vzd√°ti se ƒçinnosti jen proto, ≈æe se zd√° pan√≠ bern√≠ n√°rodn√≠ kroj pro t≈ôi dcery b√Ωti nƒõjakou zby teƒçnou v√Ωlohou?\" Paniƒçky projevovaly svoji nevoli, ka≈æd√° jin√Ωm sp≈Øsobem. Mlad√° pan√≠ adjunktov√° v duchu si umi≈àovala, ≈æe ve sv√©m p≈ô√°tel stv√≠ k ber≈àov≈Øm trochu ochladne; to tak! aby ten jejich po≈°etil√Ω n√°pad, √∫√≥inkovati p≈ôi slavnosti v obyƒçejn√©m odƒõvu, p≈ôece zv√≠tƒõzil a d√°m√°m se bylo od≈ô√≠ci tƒõch p≈Øvabn√Ωch kroj≈Ø venkovsk√Ωch, co by si jen ona, pan√≠ adjunktov√°, poƒçala s tou haldou brok√°tu, atlasu, krajek, stuh a aksamitu, za co≈æ vydala nejednu des√≠tku, utƒõ≈°uj√≠c se t√≠m, jak j√≠ to bude slu≈°eti! Hm, a ≈°kodu z toho tak√© m√≠ti nebude, mu≈æ se bude musit po nƒõkolik mƒõs√≠c≈Ø uskrovnit, slu≈æka se m√° beztoho t√©≈æ a≈æ p≈ô√≠li≈° dob≈ôe, uhrad√≠ se to na dom√°cnosti a bude! Nyn√≠ ujala se slova pan√≠ doktorka: ‚ÄûAj, od bern√≠ch to nen√≠ nic divn√©ho, pova≈æte jen: tolik dƒõt√≠! V≈ædy≈• my v≈°ecky v√≠me, ≈æe kdyby sobƒõ sleƒçny toillety samy ne ≈ô√≠dily, mnoh√©ho by nemohly m√≠ti; ony pak maj√≠ z√°sadu: nem√°-li b√Ωt nƒõco pƒõkn√©, to≈æ radƒõji nic!\" ‚ÄûPravda, ale sleƒçny Eli≈°ky, t√© nejmlad≈°√≠, jest mnƒõ l√≠to; tƒõ≈°ila se velice na selsk√Ω kroj.\" ‚ÄûBa ano, byla by v nƒõm vypadala roztomile.\" ‚ÄûNyn√≠ m√° po radosti.\" ‚ÄûInu, proƒç m√° tak nep≈ôej√≠c√≠ matinku.\" ‚ÄûTo nen√≠ to, m√° drah√°, jest v tom v≈°ak jin√Ω h√°ƒçek.\" ‚ÄûAh, ano; v≈ædy≈• v√≠me, ≈æe sotva tak tak vyjdou.\" ‚ÄûAle na knihy, kter√© jsi tv√°, jinak na≈°el byste jen kryt√≠ tƒõch mlad√Ωch moz√≠, jinak je vyd√°te.\" ‚ÄûA tak bude, ostatnƒõ jdu a≈æ na knihy; ale sly≈°et jsem od nƒõkoho, ≈æe o krojech nƒõco dƒõlat, a ≈æe to bylo bude dƒõlat n√°rodn√≠, a kdy≈æ jsi tento n√°rodn√≠ uƒçil jak, pak by mƒõl nal√©zt tuto kartu, ta by se jistƒõ mohla vymƒõnit se v≈°emi. M≈Ø≈æete-li to od n√°s odk√°zat?\" V≈°em ozdravila jej√≠ hlas, kdy≈æ ≈ôekla: ‚ÄûJ√°, j√°! a j√° ho uƒç√≠m. S tƒõmi p√°ne akademiky jsem ji≈æ mohla vypr√°vƒõt o svƒõtƒõ, v nƒõm≈æ vzn√≠t√≠ v nƒõkter√© nocy a jednoho dne vyrost√° v√°m hrozn√Ω kouzeln√Ω strom z≈Østane, o nƒõm≈æ je ps√°no: ‚ÄöU≈æ jen ≈æ√°dat!\" 50 V tuto chv√≠li koupili mohou-li v≈°ichni p√°nov√© i pan√≠, ≈æe kdy≈æ jsi to ƒçerpala, d√° se jen kupit. P≈ôi≈æili to, a j√° jim v√°m koupi uk√°≈æi.\n",
    "‚ÄûA j√° u≈æ jste t√≠m vydƒõ≈°tila. P√°nov√©, dƒõkuji v√°m za chytr√© vƒõdom√≠, se kter√Ωm v√°m projev√≠, ≈æe se u≈æ toƒç√≠ d√≠vƒç√≠ zrcadlo.\" ‚Äì 151\"\"\"\n",
    "                \n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de31d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d29c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = \"302.\"\n",
    "ocr_text = \"302.\"\n",
    "pred_text = \"302.\"\n",
    "\n",
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "021c2fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8571428571428571, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_text = 'testing'\n",
    "ocr_text = 'resting'\n",
    "pred_text = 'testing'\n",
    "\n",
    "compute_normalized_levenshtein_similarity(gt_text, ocr_text), distance(gt_text, ocr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8e22820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666674"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_improvement(compute_normalized_levenshtein_similarity(gt_text, ocr_text), \n",
    "                compute_normalized_levenshtein_similarity(gt_text, pred_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af90e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein distance: 1\n",
      "Levenshtein similarity: 0.86\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# Example strings\n",
    "s1 = \"testing\"\n",
    "s2 = \"resting\"\n",
    "\n",
    "# Calculate the Levenshtein distance\n",
    "lev_distance = Levenshtein.distance(s1, s2)\n",
    "print(f\"Levenshtein distance: {lev_distance}\")\n",
    "\n",
    "# Calculate the Levenshtein similarity\n",
    "similarity = (max(len(s1), len(s2)) - lev_distance) / max(len(s1), len(s2))\n",
    "print(f\"Levenshtein similarity: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e13bc029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       zero-shot\n",
       "1       zero-shot\n",
       "2       zero-shot\n",
       "3       zero-shot\n",
       "4       zero-shot\n",
       "          ...    \n",
       "1558    zero-shot\n",
       "1559    zero-shot\n",
       "1560    zero-shot\n",
       "1561    zero-shot\n",
       "1562    zero-shot\n",
       "Name: type, Length: 1563, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55678d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_segment_type(segment_type):\n",
    "    def postprocess(row):\n",
    "        pred_text = row[f'prediction.{segment_type}']\n",
    "        ground_text = row[f'groundtruth.{segment_type}']\n",
    "        prompt_text = row['prompt_text']\n",
    "        zero_or_few_shot = row['type']\n",
    "        \n",
    "        if pred_text is not None:\n",
    "            if type(pred_text) == str:\n",
    "                \n",
    "                if len(pred_text.strip()) > 0:\n",
    "                    if pred_text.startswith('\"'):\n",
    "                        pred_text = pred_text[1:]\n",
    "                    if pred_text.endswith('\"'):\n",
    "                        pred_text = pred_text[:-1]\n",
    "                \n",
    "                empty_prompt_text = prompt_text.replace('{{TEXT}}', '').strip()\n",
    "                \n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                prompt_text_empty = prompt_text.replace(\"{{TEXT}}\", '').strip()\n",
    "                if prompt_text_empty in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text_empty, '').strip()\n",
    "#                     print(prompt_text_empty)\n",
    "                prompt_text = prompt_text.replace(\"{{TEXT}}\", pred_text)\n",
    "                if prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(prompt_text, '').strip()\n",
    "                pred_text = pred_text.strip()\n",
    "                if empty_prompt_text in pred_text:\n",
    "                    pred_text = pred_text.replace(empty_prompt_text, '')\n",
    "                    print(empty_prompt_text)\n",
    "#                 if 'Corrected text:' in pred_text:\n",
    "#                     pred_text = pred_text.replace('Corrected text:', '')\n",
    "#                 if 'Corrected text is:' in pred_text:\n",
    "#                     pred_text = pred_text.replace('Corrected text is:', '')\n",
    "#                 if 'The corrected text:' in pred_text:\n",
    "#                     pred_text = pred_text.replace('The corrected text:', '')\n",
    "#                 if 'The corrected text is:' in pred_text:\n",
    "#                     pred_text = pred_text.replace('The corrected text is:', '')\n",
    "#                 # Cut the pred_text to the length of the ground_text\n",
    "#                 pred_text = pred_text[:len(ground_text) + len(ground_text)//4].strip()\n",
    "                \n",
    "                if 'CORRECTED TEXT:' in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"CORRECTED TEXT:\")+len(\"CORRECTED TEXT:\"):].strip()\n",
    "#                     print('The corrected text:----', pred_text)\n",
    "                    if zero_or_few_shot == 'few-shot':\n",
    "                        import pdb;pdb.set_trace()\n",
    "#                     import pdb;pdb.set_trace()\n",
    "                if 'The corrected text:' in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"The corrected text:\")+len(\"The corrected text:\"):].strip()\n",
    "#                     print('The corrected text:----', pred_text)\n",
    "#                     if zero_or_few_shot == 'few-shot':\n",
    "#                         import pdb;pdb.set_trace()\n",
    "#                     import pdb;pdb.set_trace()\n",
    "                if 'The corrected text is:' in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"The corrected text is:\")+len(\"The corrected text is:\"):].strip()\n",
    "#                     print('The corrected text is:----', pred_text)\n",
    "                if 'Corrected text:' in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"Corrected text:\")+len(\"Corrected text:\"):].strip()\n",
    "#                     print('Corrected text:----', pred_text)\n",
    "#                     if zero_or_few_shot == 'few-shot':\n",
    "#                         import pdb;pdb.set_trace()\n",
    "#                     import pdb;pdb.set_trace()\n",
    "                if 'Corrected text is:' in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"Corrected text is:\")+len(\"Corrected text is:\"):].strip()\n",
    "#                     print('Corrected text is:----', pred_text)\n",
    "                if \"The correct spelling and grammar are:\" in pred_text:\n",
    "                    pred_text = pred_text[pred_text.index(\"The correct spelling and grammar are:\")+len(\"The correct spelling and grammar are:\"):].strip()\n",
    "#                     print('The correct spelling and grammar are:----', pred_text)\n",
    "\n",
    "        return pred_text\n",
    "    return postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69664a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8bad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d9e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rv/x6hk7f3j7dzb763m4m3kgmb00000gp/T/ipykernel_9301/3664822958.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n",
    "\n",
    "bleurt.compute(references=[[\"sample text\"]], predictions=[\"sample text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89f37147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [3, 2, 1, 0],\n",
       " 'precisions': [66.66666666666667, 50.0, 50.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 3,\n",
       " 'ref_len': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleurt.compute(references=[[\"sample text\"]], predictions=[\"sample text hjgghj\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0263e",
   "metadata": {},
   "source": [
    "## Step 2: Generating LEV similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced49b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñà‚ñà‚ñà‚ñè                                                                                                                                                                                                                                           | 14/1049 [03:42<4:35:08, 15.95s/it]"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "bleurt = load_metric(\"sacrebleu\", cache_dir='cache')\n",
    "\n",
    "for idx, result in tqdm(enumerate(results), total=len(results)):\n",
    "    \n",
    "    try:\n",
    "        results[idx] = results[idx].fillna('No text')\n",
    "    except:\n",
    "        print(idx)\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    dataset_name = results[idx]['dataset_name'].unique()[0]\n",
    "    model_name = results[idx]['model'].unique()[0]\n",
    "    prompt = results[idx]['prompt'].unique()[0]\n",
    "    \n",
    "#     print('Dataset:', dataset_name, 'Model:', model_name, 'Prompt:', prompt)\n",
    "    \n",
    "    if 'icdar' in dataset_name:\n",
    "        text_types = ['sentence', 'region']\n",
    "    else:\n",
    "        text_types = ['line', 'sentence', 'region']\n",
    "    for segment_type in text_types:\n",
    "        try:\n",
    "            results[idx]['length'] = results[idx][f'groundtruth.{segment_type}'].str.len()\n",
    "            results[idx] = results[idx][results[idx]['length'] > 3]\n",
    "\n",
    "            postprocess = postprocess_segment_type(segment_type)\n",
    "            result[f'prediction.{segment_type}'] = result.apply(postprocess, axis=1)\n",
    "\n",
    "            # Compute Lev similarity\n",
    "            results[idx][f'{segment_type}-lev-ocr'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'ocr.{segment_type}']), axis=1)\n",
    "            results[idx][f'{segment_type}-lev-pred'] = \\\n",
    "                results[idx].parallel_apply(lambda x: compute_normalized_levenshtein_similarity(x[f'groundtruth.{segment_type}'],\n",
    "                                                                                     x[f'prediction.{segment_type}']), axis=1)\n",
    "\n",
    "            results[idx][f'{segment_type}-lev-improvement'] = \\\n",
    "                results[idx].parallel_apply(lambda x: get_improvement(x[f'{segment_type}-lev-ocr'],\n",
    "                                                             x[f'{segment_type}-lev-pred']), axis=1)\n",
    "            \n",
    "            # Compute Lev similarity\n",
    "            results[idx][f'{segment_type}-bleurt-ocr'] = \\\n",
    "                results[idx].parallel_apply(lambda x: bleurt.compute(references=[[x[f'groundtruth.{segment_type}']]],\n",
    "                                                                     predictions=[x[f'ocr.{segment_type}']])['score'], axis=1)\n",
    "            results[idx][f'{segment_type}-bleurt-pred'] = \\\n",
    "                results[idx].parallel_apply(lambda x: bleurt.compute(references=[[x[f'groundtruth.{segment_type}']]],\n",
    "                                                                     predictions=[x[f'prediction.{segment_type}']])['score'], axis=1)\n",
    "            \n",
    "            results[idx][f'{segment_type}-bleurt-improvement'] = \\\n",
    "                results[idx].parallel_apply(lambda x: get_improvement(x[f'{segment_type}-bleurt-ocr'],\n",
    "                                                             x[f'{segment_type}-bleurt-pred']), axis=1)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee16de0",
   "metadata": {},
   "source": [
    "## Step 3: Preparing the final results (results concatenation + generating quality bands, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7756144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(results)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f740e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "MODEL_MAP = {'gpt-4':'GPT-4', \n",
    "             'gpt-3.5-turbo':'GPT-3.5', \n",
    "             'facebook-opt-350m':'OPT-350M',\n",
    "             'bigscience-bloom-560m':'BLOOM-560M', \n",
    "             'decapoda-research-llama-7b-hf':'LLAMA-7B',\n",
    "             'davinci':'GPT-3', 'gpt2':'GPT-2', \n",
    "             'tloen-alpaca-lora-7b':'Alpaca', \n",
    "             '3few-shot-gpt-4': 'GPT-4', \n",
    "             '3few-shot-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             '3few-shot-davinci': 'GPT-3',\n",
    "             '3few-shot-gpt2': 'GPT-2', \n",
    "             '3few-shot-facebook-opt-350m': 'OPT-350M', \n",
    "             'ajmc_primary_text-gpt-4': 'GPT-4', \n",
    "             'ajmc_primary_text-bigscience-bloom-560m': 'BLOOM-560M', \n",
    "             'ajmc_primary_text-decapoda-research-llama-7b-hf': 'LLAMA-7B', \n",
    "             'ajmc_primary_text-davinci': 'GPT-3', \n",
    "             'ajmc_primary_text-facebook-opt-350m': \"OPT-350M\",\n",
    "             'ajmc_primary_text-gpt2': 'GPT-2',\n",
    "             'ajmc_primary_text-gpt-3.5-turbo': 'GPT-3.5', \n",
    "             'ajmc_mixed-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             'ajmc_mixed-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             'ajmc_mixed-gpt2': 'GPT-2',\n",
    "             'ajmc_mixed-facebook-opt-350m': 'OPT-350M',\n",
    "             'ajmc_mixed-gpt-4': 'GPT-4',\n",
    "             'ajmc_mixed-davinci': 'GPT-3',\n",
    "             'ajmc_mixed-gpt-3.5-turbo': 'GPT-3.5',\n",
    "             '3few-shot-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             '3few-shot-impresso-nzz-bigscience-bloom-560m': 'BLOOM-560M',\n",
    "             '3few-shot-impresso-nzz-decapoda-research-llama-7b-hf': 'LLAMA-7B',\n",
    "             '3few-shot-impresso-nzz-facebook-opt-350m': 'OPT-350M',\n",
    "             '3few-shot-..-..-llama-v2-llama-llama-2-7b-': 'LLAMA-2-7B',\n",
    "             '..-..-llama-v2-llama-llama-2-7b-': 'LLAMA-2-7B',\n",
    "             'facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'bigscience-bloom-3b': 'BLOOM-3B',\n",
    "             'bigscience-bloom-7b1': 'BLOOM-7.1B',\n",
    "             '3few-shot-bigscience-bloom-7b1': 'BLOOM-7.1B',\n",
    "             'ina-facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             'decapoda-research-llama-13b-hf': 'LLAMA-13B',\n",
    "             'bigscience-bloomz-3b': 'BLOOMZ-3B',\n",
    "             'bigscience-bloomz-7b1': 'BLOOMZ-7.1B',\n",
    "             'bigscience-bloomz-560m': 'BLOOMZ-560M',\n",
    "             '3few-shot-bigscience-bloom-3b': 'BLOOM-3B',\n",
    "             '3few-shot-facebook-opt-6.7b': 'OPT-6.7B',\n",
    "             '3few-shot-bigscience-bloomz-560m': 'BLOOMZ-560M',\n",
    "             '3few-shot-bigscience-bloomz-7b1': 'BLOOMZ-7.1B',\n",
    "             '3few-shot-bigscience-bloomz-3b': 'BLOOMZ-3B',\n",
    "             'meta-llama-Llama-2-7b-hf': 'LLAMA-2-7B',\n",
    "             '3few-shot-meta-llama-Llama-2-7b-hf': 'LLAMA-2-7B',\n",
    "             'decapoda-research-llama-65b-hf': 'LLAMA-65B'\n",
    "            }\n",
    "data['model'] = data['model'].apply(lambda x: MODEL_MAP[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300d6b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bbf81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46142eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Create a new column for the OCR noise level bins\n",
    "data[f\"Quality Band\"] = pd.cut(data[f'region-lev-ocr'], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e63d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[f\"Quality Band\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83313819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Overall Levenshtein Improvement\n",
    "data[f'Overall Levenshtein Improvement'] = data[[f'line-lev-improvement', \n",
    "                                                 f'sentence-lev-improvement', \n",
    "                                                 f'region-lev-improvement']].mean(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom binning function\n",
    "def bin_improvement(x):\n",
    "    if x < 0:\n",
    "        return \"Negative Improvement\"\n",
    "    elif x == 0:\n",
    "        return \"No Improvement\"\n",
    "    elif x > 0:\n",
    "        return \"Positive Improvement\"\n",
    "\n",
    "# Apply the function\n",
    "data['Improvement Band'] = data['Overall Levenshtein Improvement'].apply(bin_improvement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b541a3",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256c81a",
   "metadata": {},
   "source": [
    "## Sampling for few-shot (commented out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e6c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define five distinct quality bands\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Initialize an empty list for samples\n",
    "# all_samples = []\n",
    "\n",
    "# folder_few_shot = '../data/prompts/few_shot/'\n",
    "\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(['ina'], total=len(['ina'])):\n",
    "# # for dataset in tqdm(data['dataset_name'].unique(), total=len(data['dataset_name'].unique())):\n",
    "    \n",
    "    \n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "    \n",
    "#     print(len(data[data['dataset_name']==dataset]))\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         # Iterate over each unique prompt\n",
    "#         for prompt in prompts:\n",
    "#             sample_list = []\n",
    "            \n",
    "#             output_folder = os.path.join(folder_few_shot, dataset)\n",
    "#             if not os.path.exists(output_folder):\n",
    "#                 os.makedirs(output_folder)\n",
    "            \n",
    "#             # Repeat the sampling process until we have 3 samples\n",
    "#             while len(sample_list) < 3:\n",
    "#                 # Iterate over each unique quality band\n",
    "#                 for quality_band in quality_bands:\n",
    "                    \n",
    "#                     # Filter the data to only include rows that match the current dataset, language, prompt, and quality band\n",
    "#                     subset = data[(data['dataset_name'] == dataset) \n",
    "#                                   & (data['language'] == language)\n",
    "#                                   & (data['prompt'] == prompt)\n",
    "#                                   & (data['Quality Band'] == quality_band)\n",
    "#                                   & (data['prediction.prompt'] != 'No text')\n",
    "#                                   & (data['groundtruth.sentence'].str.len() > 10)]\n",
    "                    \n",
    "#                     # If the subset is not empty and we need more samples, take a sample\n",
    "#                     if not subset.empty and len(sample_list) < 3:\n",
    "#                         sample = subset.sample(1, random_state=1)\n",
    "# #                         print(sample)\n",
    "#                         sample_list.append(sample)\n",
    "#                 # Break the loop if we already have 3 samples\n",
    "#                 if len(sample_list) >= 3:\n",
    "#                     break\n",
    "\n",
    "#             all_samples.extend(sample_list)\n",
    "#             if 'icdar' in dataset_name:\n",
    "#                 text_types = ['sentence', 'region']\n",
    "#             else:\n",
    "#                 text_types = ['line', 'sentence', 'region']\n",
    "            \n",
    "# #             Generating prompts\n",
    "#             print(prompt)\n",
    "#             for segment_type in text_types:\n",
    "#                 output_file = os.path.join(output_folder, f'{prompt}_{segment_type}_{language}.txt')\n",
    "#                 with open(output_file, 'w') as f:\n",
    "#                     for sample in sample_list:\n",
    "#                         sample = sample.iloc[0]\n",
    "#                         prompt_text = sample['prompt_text'].replace('{{TEXT}}', sample[f'ocr.{segment_type}'])\n",
    "#                         correct_text = sample[f'groundtruth.{segment_type}']\n",
    "#                         f.write(f'{prompt_text}\\n\\n{correct_text}\\n\\n')\n",
    "#                         f.write(sample['prompt_text'])\n",
    "\n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(all_samples, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e8925",
   "metadata": {},
   "source": [
    "## Sampling for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4147f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# dataset_names = data.dataset_name.unique()\n",
    "# quality_bands = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# sample_list = []\n",
    "# # Iterate over all unique datasets\n",
    "# for dataset in tqdm(dataset_names, total = len(dataset_names)):\n",
    "#     # Get the unique languages for the current dataset\n",
    "#     languages = data[data['dataset_name'] == dataset]['language'].unique()\n",
    "#     print(dataset)\n",
    "#     # Iterate over each unique language\n",
    "#     for language in languages:\n",
    "#         print('  --', language)\n",
    "#         groundtruth_samples = data[(data['dataset_name'] == dataset) \n",
    "#                                    & (data['language'] == language)\n",
    "#                                    & (data['groundtruth.sentence'].str.len() > 10)].drop_duplicates(subset=['groundtruth.line', 'groundtruth.sentence', 'groundtruth.region'])\n",
    "#         # Limit the groundtruth_samples to three\n",
    "#         if len(groundtruth_samples) >= 3:\n",
    "#             groundtruth_samples = groundtruth_samples.sample(3, random_state=1335)\n",
    "\n",
    "            \n",
    "#         print(len(groundtruth_samples))             \n",
    "#         # Iterate over each unique groundtruth samples\n",
    "#         for idx, gt_sample in groundtruth_samples.iterrows():\n",
    "#             prompts = data[data['dataset_name'] == dataset]['prompt'].unique()\n",
    "                           \n",
    "#             models = data[data['dataset_name'] == dataset]['model'].unique()\n",
    "                          \n",
    "#             improvement_bands = data[data['dataset_name'] == dataset]['Improvement Band'].unique()\n",
    "#             is_few_shot_or_not = data[data['dataset_name'] == dataset]['type'].unique()\n",
    "\n",
    "#             # Iterate over each unique prompt\n",
    "#             for prompt in prompts:\n",
    "#                 print('    -', prompt)\n",
    "#                 # Iterate over each unique model\n",
    "#                 for model in models:\n",
    "# #                     print('     --', model)\n",
    "#                     # Iterate over each quality band\n",
    "#                     for band in quality_bands:\n",
    "# #                         print('        ---', band)\n",
    "#                         # Iterate over each improvement band\n",
    "#                         for improvement_band in improvement_bands:\n",
    "# #                             print('          ----', improvement_band)\n",
    "#                             for is_few_shot in is_few_shot_or_not:\n",
    "# #                                 print('-------', is_few_shot)\n",
    "#                                 subset = data[(data['dataset_name'] == dataset) \n",
    "#                                               & (data['language'] == language)\n",
    "#                                               & (data['prompt'] == prompt)\n",
    "#                                               & (data['model'] == model)\n",
    "#                                               & (data['Improvement Band'] == improvement_band)\n",
    "#                                               & (data['Quality Band'] == band)\n",
    "#                                               & (data['type'] == is_few_shot)\n",
    "#                                               & (data['groundtruth.line'] == gt_sample['groundtruth.line'])\n",
    "#                                               & (data['groundtruth.sentence'] == gt_sample['groundtruth.sentence'])\n",
    "#                                               & (data['groundtruth.region'] == gt_sample['groundtruth.region'])]\n",
    "                                          \n",
    "#                                 # If the subset is not empty, take a sample\n",
    "#                                 if not subset.empty:\n",
    "#                                     sample = subset.sample(1, random_state=1, replace=True)\n",
    "#                                     sample_list.append(sample)\n",
    "#     #                                 print(sample)\n",
    "#     #                             else:\n",
    "#     #                                 print(f\"No samples for Dataset: {dataset}, Language: {language}\")\n",
    "\n",
    "                                      \n",
    "# # Concatenate all the samples into a single DataFrame\n",
    "# sample_df = pd.concat(sample_list, ignore_index=True)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27599ac0",
   "metadata": {},
   "source": [
    "### Order columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d91bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_df = sample_df.drop(['length', 'NbAlignedChar', 'prompt_text', 'File'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd36acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 'index', \n",
    "# order = ['filename', 'dataset_name', 'model', 'language', 'prompt', \n",
    "#          'Overall Levenshtein Improvement', 'Quality Band', 'Improvement Band',\n",
    "#          'ocr.line', 'groundtruth.line', 'prediction.line', \n",
    "#          'line-lev-ocr', 'line-lev-pred', 'line-lev-improvement',\n",
    "#          'ocr.sentence', 'groundtruth.sentence', 'prediction.sentence', \n",
    "#          'sentence-lev-ocr', 'sentence-lev-pred', 'sentence-lev-improvement', \n",
    "#          'ocr.region', 'groundtruth.region', 'prediction.region',\n",
    "#          'region-lev-ocr', 'region-lev-pred', 'region-lev-improvement', \n",
    "#          'article_id', 'century', 'Date', 'Type']\n",
    "\n",
    "# # Reorder the DataFrame\n",
    "# sample_df = sample_df[order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea396a1d",
   "metadata": {},
   "source": [
    "### Write sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Use today's date for the filename\n",
    "# today = datetime.now().strftime('%d%B')  # This will format the date as 'DayMonth'\n",
    "\n",
    "# # Save the DataFrame to a csv file\n",
    "# sample_df.to_csv(f'ResultsGPTUpdated{today}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff79fc4",
   "metadata": {},
   "source": [
    "### Distribution of WER/CER rates for all datasets in the four quality bands, established via Levenshtein similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb6600",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define the bins and labels for quality bands\n",
    "# bins = [0, 0.7, 0.8, 0.9, 1]\n",
    "# labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['cer', 'wer']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Mean {error_rate.upper()}'] = dataset_data[[f'line-{error_rate}-ocr', \n",
    "#                                                  f'sentence-{error_rate}-ocr', \n",
    "#                                                  f'region-{error_rate}-ocr']].mean(axis=1)\n",
    "\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Mean {error_rate.upper()}\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "#         axs[i].set_xlim([0, 100])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         _ = fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f'Mean {error_rate.upper()} Error Rates across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # Define OCR noise level bins\n",
    "# # bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# # bins = [0, 0.7, 0.8, 0.9, 1.0]\n",
    "# bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# # Assign OCR noise level labels\n",
    "# # labels = [\"0-10%\", \"10-20%\", \"20-30%\", \"30-40%\", \"40-50%\", \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# # labels = [\"0-70%\", \"70-80%\", \"80-90%\", \"90-100%\"]\n",
    "# labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# # Count the number of unique datasets\n",
    "# n_datasets = data.dataset_name.nunique()\n",
    "# dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "#                  'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec']\n",
    "\n",
    "# for error_rate in ['lev']:\n",
    "#     # Create subplots\n",
    "#     fig, axs = plt.subplots(3, 4, figsize=(20, 15))\n",
    "\n",
    "#     # Flatten the axes for easy iteration\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     for i, dataset in enumerate(dataset_names):\n",
    "#         dataset_data = data[data.dataset_name == dataset]\n",
    "\n",
    "#         # Compute the mean WER across line, sentence, and region levels\n",
    "#         dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "#                                                           f'sentence-{error_rate}-improvement', \n",
    "#                                                           f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "#         # Plot the distribution of WERs for each quality band\n",
    "#         for band in labels:\n",
    "#             band_df = dataset_data[dataset_data[f\"{segment_type}-ocr-noise-group\"] == band]\n",
    "\n",
    "#             _ = sns.histplot(band_df, x=f\"Overall Levenshtein Improvement\", \n",
    "#                              label=f\"Quality Band {band}\", kde=True, ax=axs[i])\n",
    "\n",
    "# #         axs[i].set_ylim([0, 300])\n",
    "#         axs[i].set_xlim([-1, 1])\n",
    "#         axs[i].set_title(f'{dataset.upper()}')\n",
    "#         axs[i].legend()\n",
    "\n",
    "#     # Remove empty subplots\n",
    "#     for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "#         fig.delaxes(axs[i])\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle('Overall Levenshtein Improvement across Datasets and Quality Bands', fontsize=20, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.type.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.prompt.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Improvement Band'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['model'] != 'LLaMA-13B']\n",
    "data = data.loc[data['model'] != 'LLAMA-65B']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.model.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "       'LLAMA-7B', 'LLAMA-2-7B', \n",
    "       'BLOOM-560M',  'BLOOM-3B',  'BLOOM-7.1B', \n",
    "       'BLOOMZ-560M', 'BLOOMZ-3B', 'BLOOMZ-7.1B', \n",
    "       'OPT-350M', 'OPT-6.7B',\n",
    "       'GPT-2', 'GPT-3', 'GPT-3.5', 'GPT-4']\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a91c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_models = ['LLAMA-7B', 'LLAMA-2-7B', \n",
    "       'BLOOM-560M',  'BLOOM-3B',  'BLOOM-7.1B', \n",
    "       'BLOOMZ-560M', 'BLOOMZ-3B', 'BLOOMZ-7.1B', \n",
    "       'OPT-350M', 'OPT-6.7B',\n",
    "       'GPT-2']  # Replace these with your list of 'limited' models\n",
    "open_models = ['GPT-3', 'GPT-3.5', 'GPT-4']  # Replace these with your list of 'open' models\n",
    "\n",
    "# Create new 'Access' column\n",
    "data['Access'] = data['model'].apply(lambda x: 'limited' if x in limited_models else ('open' if x in open_models else 'unknown'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2079a585",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ad365",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "print(n_figures)\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                      f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1, 1.2])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91890cfa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:#\n",
    "    for error_rate in ['lev']:\n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(17, 9))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                    \n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                \n",
    "                dataset_data['model'] = pd.Categorical(dataset_data['model'], categories=MODELS, ordered=True)\n",
    "                \n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                      f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the line plot of improvements for each model\n",
    "                    _ = sns.lineplot(x='model', y=f'Overall Levenshtein Improvement', \n",
    "                                     data=dataset_data.sort_values('model'), \n",
    "                                    ax=axs[i], hue='prompt', sort=False, linewidth=2)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "                    axs[i].set_ylim([-1, 1.2])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)\n",
    "                    axs[i].set_xlabel('Prompt Complexity')  \n",
    "                    axs[i].set_ylabel('Overall Levenshtein Improvement')  \n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Evolution of Prompt Complexity across Datasets, Models, and Improvements ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69825",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each language and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'language', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for language in data.language.unique():  # Iterate over each unique language\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.language == language) & (data.type == type_of_experiment)]  # Filter data for the current language\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and language, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and language again, this time averaging the averages for each language and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the line plot of average improvements for each prompt\n",
    "            _ = sns.lineplot(x='prompt', y=f'Overall Levenshtein Improvement', hue='language', data=average_improvements, \n",
    "                             ax=ax, legend='full')  # Add hue='language' to differentiate lines by language\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement ({type_of_experiment})')  # Modify title since it's no longer specific to one language\n",
    "        ax.set_ylim([-1, 1.2])\n",
    "        ax.set_xlabel('Prompt Complexity')  \n",
    "        ax.set_ylabel('Average Overall Levenshtein Improvement')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each language and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'language', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for language in data.language.unique():  # Iterate over each unique language\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.language == language) & (data.type == type_of_experiment)]  # Filter data for the current language\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and language, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and language again, this time averaging the averages for each language and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'language'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the KDE plot of average improvements for each prompt\n",
    "            for language in average_improvements.language.unique():\n",
    "                _ = sns.kdeplot(average_improvements[average_improvements.language == language][f'Overall Levenshtein Improvement'], \n",
    "                                ax=ax, label=language, lw=2.5)  # Increase line width here\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement KDE ({type_of_experiment})')  # Modify title since it's no longer specific to one language\n",
    "        ax.set_xlabel('Average Overall Levenshtein Improvement')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.legend(title='Language', title_fontsize='13', fontsize='12')  # Increase legend fontsize here\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement KDE ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        # Create a new DataFrame to store the average improvements for each model and prompt\n",
    "        average_improvements = pd.DataFrame(columns=['prompt', 'model', f'Overall Levenshtein Improvement'])\n",
    "\n",
    "        for model in data.model.unique():  # Iterate over each unique model\n",
    "            for dataset in dataset_names:\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.model == model) & (data.type == type_of_experiment)]  # Filter data for the current model\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                # Group the data by prompt and model, then calculate the mean Overall Levenshtein Improvement\n",
    "                grouped_data = dataset_data.groupby(['prompt', 'model'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "                # Add the grouped data to the average_improvements DataFrame\n",
    "                average_improvements = pd.concat([average_improvements, grouped_data])\n",
    "\n",
    "        # Group the data by prompt and model again, this time averaging the averages for each model and prompt\n",
    "        average_improvements = average_improvements.groupby(['prompt', 'model'])[f'Overall Levenshtein Improvement'].mean().reset_index()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))  # Create the plot\n",
    "\n",
    "        try:\n",
    "            # Plot the KDE plot of average improvements for each prompt\n",
    "            for model in average_improvements.model.unique():\n",
    "                _ = sns.kdeplot(average_improvements[average_improvements.model == model][f'Overall Levenshtein Improvement'], \n",
    "                                ax=ax, label=model, lw=2.5)  # Increase line width here\n",
    "        except Exception as ex:\n",
    "            print(f'Could not load data with {ex}')\n",
    "\n",
    "        ax.set_title(f'Average Overall Levenshtein Improvement KDE ({type_of_experiment})')  # Modify title since it's no longer specific to one model\n",
    "        ax.set_xlabel('Average Overall Levenshtein Improvement')\n",
    "        ax.set_ylabel('Density')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.legend(title='Model', title_fontsize='13', fontsize='12')  # Increase legend fontsize here\n",
    "        plt.suptitle(f'Evolution of Prompt Complexity and Average Improvement KDE ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b6985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define OCR noise level bins\n",
    "bins = [0, 0.4, 0.6, 0.8, 0.99, 1.0]\n",
    "\n",
    "# Assign OCR noise level labels\n",
    "labels = [\"0-40%\", \"40-60%\", \"60-80%\", \"80-99%\", \"99-100%\"]\n",
    "\n",
    "# Count the number of unique datasets\n",
    "n_datasets = data.dataset_name.nunique()\n",
    "dataset_names = ['impresso-nzz', 'overproof', 'ajmc-mixed', \n",
    "                 'ajmc-primary-text', 'icdar-2017', 'icdar-2019', 'htrec', 'ina']\n",
    "\n",
    "prompt_names = ['prompt_basic_01', 'prompt_basic_02', 'prompt_complex_01', \n",
    "                'prompt_complex_02', 'prompt_complex_lang']\n",
    "\n",
    "\n",
    "n_plots = len(dataset_names)\n",
    "n_plots_per_figure = 4\n",
    "n_figures = int(np.ceil(n_plots / n_plots_per_figure))\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "# for type_of_experiment in ['language-specific']:\n",
    "    for error_rate in ['lev']:\n",
    "        \n",
    "        for fig_idx in range(n_figures):\n",
    "            fig, axs = plt.subplots(2, 2, figsize=(30, 15))\n",
    "            axs = axs.flatten()\n",
    "\n",
    "            for i in range(n_plots_per_figure):\n",
    "                idx = fig_idx * n_plots_per_figure + i\n",
    "                if idx < n_plots:\n",
    "                    dataset = dataset_names[idx]\n",
    "                dataset_data = data[(data.dataset_name == dataset) & (data.type == type_of_experiment)]\n",
    "                # Compute the mean WER across line, sentence, and region levels\n",
    "                if 'icdar' not in dataset:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    dataset_data[f'Overall Levenshtein Improvement'] = dataset_data[[f'sentence-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "                try:\n",
    "                    # Plot the distribution of improvements for each model\n",
    "                    _ = sns.boxplot(x='model', y=f'Overall Levenshtein Improvement', data=dataset_data, \n",
    "                                    ax=axs[i], order=MODELS, hue='prompt', hue_order=prompt_names)\n",
    "                    axs[i].set_title(f'{dataset.upper()} ({type_of_experiment})')\n",
    "\n",
    "                    axs[i].set_ylim([-1.1, 1.1])\n",
    "                    axs[i].set_xticklabels(axs[i].get_xticklabels(), rotation=15)  # Rotate x-axis labels\n",
    "                    axs[i].set_xlabel('')  # Remove x-axis label\n",
    "                    axs[i].set_ylabel('')  # Remove y-axis label\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not load {dataset} with {ex}')\n",
    "\n",
    "\n",
    "            # Remove empty subplots\n",
    "            for i in range(len(data.dataset_name.unique()), len(axs)):\n",
    "                fig.delaxes(axs[i])\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.suptitle(f'Overall Levenshtein Improvement across Datasets, Models, and Prompts ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfe884",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=.7)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "        axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "        for i, prompt in enumerate(prompt_names):\n",
    "            prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "            \n",
    "            if 'icdar' not in dataset:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                prompt_data[f'Overall Levenshtein Improvement'] = prompt_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(prompt_data) > 0:\n",
    "                    sns.kdeplot(data=prompt_data, x=f'Overall Levenshtein Improvement', hue='model', \n",
    "                                fill=True, ax=axs[i], hue_order=MODELS)\n",
    "                    axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                    axs[i].set_xlim([-1, 1.2])\n",
    "                    axs[i].set_ylim([0, 1.4])\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "        # Remove empty subplot\n",
    "        fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, \n",
    "# analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more \n",
    "# dimensions.\n",
    "\n",
    "# Relative to a histogram, KDE can produce a plot that is less cluttered and more interpretable, especially when \n",
    "# drawing multiple distributions. But it has the potential to introduce distortions if the underlying distribution \n",
    "# is bounded or not smooth. Like a histogram, the quality of the representation also depends on the selection of \n",
    "# good smoothing parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd61e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        plt.figure(figsize=(10, 6))  # Adjust as necessary\n",
    "\n",
    "        for language in languages:\n",
    "            language_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt.isin(prompts_to_compare))]\n",
    "\n",
    "            if 'icdar' not in dataset:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'line-{error_rate}-improvement', \n",
    "                                                                  f'sentence-{error_rate}-improvement', \n",
    "                                                                  f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                language_data[f'Overall Levenshtein Improvement'] = language_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                              f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "\n",
    "            try:\n",
    "                if len(language_data) > 0:\n",
    "                    sns.kdeplot(data=language_data, x=f'Overall Levenshtein Improvement', hue='prompt', \n",
    "                                fill=True, hue_order=prompts_to_compare)\n",
    "            except Exception as ex:\n",
    "                print(f'Could not plot {language} with {ex}')\n",
    "\n",
    "        plt.title(f'All Languages ({type_of_experiment})')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22961951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=1.)\n",
    "\n",
    "\n",
    "prompts_to_compare = ['prompt_complex_02', 'prompt_complex_lang']\n",
    "languages = [lang for lang in data['language'].unique() if lang != 'en']  # Exclude 'en' language\n",
    "\n",
    "bar_data = []\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for language in languages:\n",
    "            for prompt in prompts_to_compare:\n",
    "                sub_data = data[(data.language == language) & \n",
    "                                 (data.type == type_of_experiment) & \n",
    "                                 (data.prompt == prompt)]\n",
    "                if 'icdar' not in dataset:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'line-{error_rate}-improvement', \n",
    "                                                                          f'sentence-{error_rate}-improvement', \n",
    "                                                                          f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                else:\n",
    "                    sub_data[f'Overall Levenshtein Improvement'] = sub_data[[f'sentence-{error_rate}-improvement', \n",
    "                                                                      f'region-{error_rate}-improvement']].mean(axis=1)\n",
    "                \n",
    "                mean_improvement = np.mean(sub_data[f'Overall Levenshtein Improvement'])\n",
    "                bar_data.append({'Language': language, 'Prompt': prompt, 'Mean Improvement': mean_improvement})\n",
    "\n",
    "bar_data = pd.DataFrame(bar_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Language', y='Mean Improvement', hue='Prompt', data=bar_data, hue_order=prompts_to_compare)\n",
    "plt.title(f'Mean Levenshtein Improvement for all Languages ({type_of_experiment})')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde994d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_context(\"notebook\", font_scale=.7)\n",
    "\n",
    "segmentations = ['line', 'sentence', 'region']\n",
    "\n",
    "for type_of_experiment in ['zero-shot']:\n",
    "    for error_rate in ['lev']:\n",
    "        for segmentation in segmentations:\n",
    "            fig, axs = plt.subplots(3, 2, figsize=(8, 12))  # Change here\n",
    "            axs = axs.flatten()  # To make it easy to index\n",
    "\n",
    "            for i, prompt in enumerate(prompt_names):\n",
    "                prompt_data = data[(data.prompt == prompt) & (data.type == type_of_experiment)]\n",
    "                if segmentation == 'line':\n",
    "                    prompt_data = prompt_data[~prompt_data.dataset_name.isin(['icdar-2017', 'icdar-2019'])]\n",
    "                prompt_data[f'{segmentation.capitalize()} Levenshtein Improvement'] = prompt_data[f'{segmentation}-{error_rate}-improvement']\n",
    "\n",
    "                try:\n",
    "                    if len(prompt_data) > 0:\n",
    "                        sns.kdeplot(data=prompt_data, x=f'{segmentation.capitalize()} Levenshtein Improvement', \n",
    "                                    hue='model', fill=True, ax=axs[i], hue_order=MODELS)\n",
    "                        axs[i].set_title(f'{prompt} ({type_of_experiment})')\n",
    "                        axs[i].set_xlim([-1, 1.2])\n",
    "                        axs[i].set_ylim([0, 1.5])\n",
    "                except Exception as ex:\n",
    "                    print(f'Could not plot {prompt} with {ex}')\n",
    "\n",
    "            # Remove empty subplot\n",
    "            fig.delaxes(axs[-1])  # Change here\n",
    "\n",
    "            plt.suptitle(f'{segmentation.capitalize()} Levenshtein Improvement across Prompts and Models ({type_of_experiment})', fontsize=20, y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e54da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "results = []  # Define results as a list\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    for model in MODELS:\n",
    "        model_data = data[(data.model == model) & (data.type == type_of_experiment)]\n",
    "        \n",
    "        # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "        if 'icdar' not in dataset:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'line-lev-improvement', \n",
    "                                                                        f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "        else:\n",
    "            model_data['Overall Levenshtein Improvement'] = model_data[[f'sentence-lev-improvement', \n",
    "                                                                        f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "        # Append the results\n",
    "#         print(model_data['Improvement Band'].unique())\n",
    "        results.append({'Model': model,\n",
    "                        'Type of Experiment': type_of_experiment,\n",
    "                        'Overall Levenshtein Improvement': np.nanmean(model_data['Overall Levenshtein Improvement'])})\n",
    "\n",
    "# Convert the results list to a DataFrame for plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Type of Experiment', data=results_df, order=MODELS)\n",
    "plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba62497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for type_of_experiment in ['zero-shot', 'few-shot']:\n",
    "    results = []  # Define results as a list\n",
    "\n",
    "    for model in MODELS:\n",
    "        for band in data['Improvement Band'].unique():\n",
    "            model_band_data = data[(data.model == model) & (data.type == type_of_experiment) & (data['Improvement Band'] == band)]\n",
    "            \n",
    "            # Compute the mean Levenshtein Improvement across line, sentence, and region levels\n",
    "            if 'icdar' not in dataset:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'line-lev-improvement', \n",
    "                                                                                    f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "            else:\n",
    "                model_band_data['Overall Levenshtein Improvement'] = model_band_data[[f'sentence-lev-improvement', \n",
    "                                                                                    f'region-lev-improvement']].mean(axis=1)\n",
    "\n",
    "            # Append the results\n",
    "            results.append({'Model': model,\n",
    "                            'Type of Experiment': type_of_experiment,\n",
    "                            'Overall Levenshtein Improvement': np.nanmean(model_band_data['Overall Levenshtein Improvement']),\n",
    "                            'Improvement Band': band})\n",
    "\n",
    "    # Convert the results list to a DataFrame for plotting\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x='Model', y='Overall Levenshtein Improvement', hue='Improvement Band', \n",
    "                data=results_df, order=MODELS)\n",
    "    plt.title('Overall Levenshtein Improvement for Zero-Shot and Few-Shot Models')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(data, col='Type', hue='Improvement Band', height=10, aspect=1)\n",
    "g.map(sns.barplot, 'model', 'Overall Levenshtein Improvement', order=MODELS)\n",
    "\n",
    "# Calculate means for each type of experiment and add horizontal lines\n",
    "for ax, (type_of_experiment, item) in zip(g.axes.flatten(), data.groupby('Type')):\n",
    "    mean_improvement = item['Overall Levenshtein Improvement'].mean()\n",
    "    ax.axhline(mean_improvement, color='black', linestyle='--')\n",
    "    ax.text(0.6, mean_improvement, f'Mean: {mean_improvement:.2f}', color='black')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68913b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818014ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449b6097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff5018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d142c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
